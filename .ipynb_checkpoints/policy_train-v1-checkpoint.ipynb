{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:20.862421Z",
     "start_time": "2019-12-16T15:33:19.175073Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data\\SPFB.RTS_090601_190813.txt',sep=',')\n",
    "df['Timestamp'] = df[\"<DATE>\"].astype(str) + df[\"<TIME>\"].astype(str)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'],format='%Y%m%d%H%M%S')\n",
    "df.set_index('Timestamp',inplace=True)\n",
    "df.drop(['<DATE>','<TIME>'],axis=1,inplace=True)\n",
    "df.columns = ['Open','High','Low','Close','Vol']\n",
    "df.drop('Vol',axis=1,inplace=True)\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:20.986693Z",
     "start_time": "2019-12-16T15:33:20.929817Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: how in .resample() is deprecated\n",
      "the new syntax is .resample(...)..apply(<func>)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "conversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last'}\n",
    "df=df.resample('30Min', how=conversion)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:21.142277Z",
     "start_time": "2019-12-16T15:33:21.047530Z"
    }
   },
   "outputs": [],
   "source": [
    "def bar_norm(df):\n",
    "    t2=df.copy()\n",
    "    temp=df.copy()\n",
    "    temp['dif_close_1_cum']=(temp['Close']-temp['Close'].shift()).shift().cumsum()\n",
    "    t2['Open_std']=t2['Open'] - temp['dif_close_1_cum']\n",
    "    t2['High_std']=t2['High'] - temp['dif_close_1_cum']\n",
    "    t2['Low_std']=t2['Low'] - temp['dif_close_1_cum']\n",
    "    t2['Close_std']=t2['Close'] - temp['dif_close_1_cum']\n",
    "    t2['Open_std']=t2['Open_std'] - temp['Close'][0]\n",
    "    t2['High_std']=t2['High_std'] - temp['Close'][0]\n",
    "    t2['Low_std']=t2['Low_std'] - temp['Close'][0]\n",
    "    t2['Close_std']=t2['Close_std'] - temp['Close'][0]\n",
    "    #t2=t2-temp['Close'][0]\n",
    "    t2=t2.iloc[2:]\n",
    "    return t2\n",
    "\n",
    "def bar_norm_all(df):\n",
    "    df=bar_norm(df)\n",
    "    df['std']=df['Close_std'].rolling(1250).std()\n",
    "    df.dropna(inplace=True)\n",
    "    df['Open_std']=df['Open_std']/df['std']\n",
    "    df['High_std']=df['High_std']/df['std']\n",
    "    df['Low_std']=df['Low_std']/df['std']\n",
    "    df['Close_std']=df['Close_std']/df['std']\n",
    "    df.drop('std',axis=1,inplace=True)\n",
    "    return df\n",
    "df=bar_norm_all(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:21.208073Z",
     "start_time": "2019-12-16T15:33:21.204084Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df[-70000:]\n",
    "df_train=df[:-30000]\n",
    "df_valid=df[-30000:-15000]\n",
    "df_test=df[-15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:21.290852Z",
     "start_time": "2019-12-16T15:33:21.272900Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_RTStrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:21.456737Z",
     "start_time": "2019-12-16T15:33:21.450751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Trade-v1')\n",
    "env.data_init(df_train,df_valid,df_test,df_test.iloc[:,:4])\n",
    "obs=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:23.654160Z",
     "start_time": "2019-12-16T15:33:22.019615Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "\n",
    "def get_policy_model(lr,dimen=(40,4)):\n",
    "    inp = layers.Input(shape=(40,4),name=\"input_x\")\n",
    "    adv = layers.Input(shape=[1], name=\"advantages\")\n",
    "    \n",
    "    x = layers.Conv1D (16,3,activation='relu')(inp)\n",
    "    x = layers.Conv1D (24,3,activation='relu')(x)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Conv1D (32,3,activation='relu')(x)\n",
    "    x = layers.Conv1D (48,3,activation='relu')(x)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(30,activation='relu')(x)\n",
    "    out = layers.Dense(3,activation='softmax')(x)\n",
    "    \n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        log_lik = K.log(y_true * (y_true - y_pred) + (1 - y_true) * (y_true + y_pred) + K.epsilon())\n",
    "        return K.mean(log_lik * adv, keepdims=True)\n",
    "        \n",
    "    model_train = Model(inputs=[inp, adv], outputs=out)\n",
    "    model_train.compile(loss=custom_loss, optimizer=Adam(lr))\n",
    "    model_predict = Model(inputs=[inp], outputs=out)\n",
    "    return model_train, model_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:23.735264Z",
     "start_time": "2019-12-16T15:33:23.732248Z"
    }
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.8):\n",
    "    prior = 0\n",
    "    out = []\n",
    "    for val in r[::-1]:\n",
    "        new_val = val + prior * gamma\n",
    "        out.append(new_val)\n",
    "        prior = new_val\n",
    "    return np.array(out[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:23.927359Z",
     "start_time": "2019-12-16T15:33:23.915391Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_train_model(model):   \n",
    "    obs = env.reset_to_trainScore()\n",
    "    obs_v=obs.values\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()        \n",
    "        state = np.reshape(obs_v, [1, 40, 4])\n",
    "        predict = model.predict([state])[0]\n",
    "        action = np.argmax(predict)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs_v=obs.values\n",
    "        if done:\n",
    "            print('done')\n",
    "            break\n",
    "    \n",
    "    return (env.net,env.action_count)\n",
    "\n",
    "def score_valid_model(model):\n",
    "    obs = env.reset_to_validScore()\n",
    "    obs_v=obs.values\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()        \n",
    "        state = np.reshape(obs_v, [1, 40, 4])\n",
    "        predict = model.predict([state])[0]\n",
    "        action = np.argmax(predict)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs_v=obs.values\n",
    "        if done:\n",
    "            print('done')\n",
    "            break\n",
    "    return (env.net,env.action_count)\n",
    "\n",
    "def score_test_model(model):\n",
    "    obs = env.reset_to_testScore()\n",
    "    obs_v=obs.values\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()        \n",
    "        state = np.reshape(obs_v, [1, 40, 4])\n",
    "        predict = model.predict([state])[0]\n",
    "        action = np.argmax(predict)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs_v=obs.values\n",
    "        if done:\n",
    "            print('done')\n",
    "            break\n",
    "    return (env.net,env.action_count)\n",
    "\n",
    "def score_testReal_model(model):\n",
    "    obs = env.reset_to_testReal()\n",
    "    obs = bar_norm_all(obs)\n",
    "    obs=obs.iloc[-40:,4:]\n",
    "    obs_v=obs.values\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()        \n",
    "        state = np.reshape(obs_v, [1, 40, 4])\n",
    "        predict = model.predict([state])[0]\n",
    "        action = np.argmax(predict)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs = bar_norm_all(obs)\n",
    "        obs=obs.iloc[-40:,4:]\n",
    "        obs_v=obs.values\n",
    "        if done:\n",
    "            print('done')\n",
    "            break\n",
    "    return (env.net,env.action_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:26.190579Z",
     "start_time": "2019-12-16T15:33:26.187587Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gamma = 0\n",
    "dimen = (40,4)\n",
    "print_every = 300\n",
    "batch_size = 100\n",
    "num_episodes = 15000\n",
    "render = False\n",
    "lr = 1e-3\n",
    "print_every_train = print_every*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:26.839682Z",
     "start_time": "2019-12-16T15:33:26.707621Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0110 12:39:12.607199  7216 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0110 12:39:12.772193  7216 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0110 12:39:12.782230  7216 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0110 12:39:12.850183  7216 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0110 12:39:12.932157  7216 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0110 12:39:12.939159  7216 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_x (InputLayer)         (None, 40, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 38, 16)            208       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 36, 24)            1176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 18, 24)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16, 32)            2336      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 14, 48)            4656      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 7, 48)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 336)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                10110     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 18,579\n",
      "Trainable params: 18,579\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train, model_predict = get_policy_model(lr,dimen)\n",
    "model_predict.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T19:08:57.973244Z",
     "start_time": "2019-12-16T15:33:29.196773Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0110 12:39:12.966156  7216 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 300: -686.43 Train Score: 0.00 ;0,0,0 Loss: 0.000543 Valid Score: -38645.00 ;4596,10185,179 TEST Score: \u001b[47m \u001b[37m  -53277.00 ;4742,10037,181  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 600: -752.53 Train Score: 0.00 ;0,0,0 Loss: 0.000564 Valid Score: -7393.00 ;1245,13573,142 TEST Score: \u001b[47m \u001b[37m  -32540.00 ;1338,13460,162  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 900: -1087.91 Train Score: 0.00 ;0,0,0 Loss: 0.000687 Valid Score: -18250.00 ;124,14740,96 TEST Score: \u001b[47m \u001b[37m  -20196.00 ;166,14686,108  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 1200: -340.76 Train Score: 0.00 ;0,0,0 Loss: 0.000593 Valid Score: -13475.00 ;83,14855,22 TEST Score: \u001b[47m \u001b[37m  -18868.00 ;128,14808,24  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 1500: -895.76 Train Score: 4888.00 ;414,39522,24 Loss: 0.000595 Valid Score: -16185.00 ;124,14825,11 TEST Score: \u001b[47m \u001b[37m  -20810.00 ;159,14790,11  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 1800: -335.23 Train Score: 0.00 ;0,0,0 Loss: 0.000534 Valid Score: -11405.00 ;166,14745,49 TEST Score: \u001b[47m \u001b[37m  -22968.00 ;206,14708,46  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 2100: -126.36 Train Score: 0.00 ;0,0,0 Loss: 0.000449 Valid Score: 3004.00 ;1283,13576,101 TEST Score: \u001b[47m \u001b[37m  -34289.00 ;1415,13439,106  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 2400: -257.05 Train Score: 0.00 ;0,0,0 Loss: 0.000390 Valid Score: -73.00 ;3985,10443,532 TEST Score: \u001b[47m \u001b[37m  -16377.00 ;4101,10367,492  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 2700: -404.62 Train Score: 0.00 ;0,0,0 Loss: 0.000361 Valid Score: 11333.00 ;4528,6897,3535 TEST Score: \u001b[47m \u001b[37m  25387.00 ;4590,7063,3307  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 3000: 93.66 Train Score: 602063.00 ;20850,4422,14688 Loss: 0.000261 Valid Score: -10286.00 ;7691,1596,5673 TEST Score: \u001b[47m \u001b[37m  27121.00 ;7713,1739,5508  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 3300: 293.07 Train Score: 0.00 ;0,0,0 Loss: 0.000052 Valid Score: 21264.00 ;10518,16,4426 TEST Score: \u001b[47m \u001b[37m  23728.00 ;10635,22,4303  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 3600: 956.90 Train Score: 0.00 ;0,0,0 Loss: -0.000281 Valid Score: 11880.00 ;12691,0,2269 TEST Score: \u001b[47m \u001b[37m  8680.00 ;12809,0,2151  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 3900: 765.98 Train Score: 0.00 ;0,0,0 Loss: -0.000732 Valid Score: 25280.00 ;12988,0,1972 TEST Score: \u001b[47m \u001b[37m  -2520.00 ;13103,0,1857  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 4200: 956.06 Train Score: 0.00 ;0,0,0 Loss: -0.001597 Valid Score: 18990.00 ;13244,0,1716 TEST Score: \u001b[47m \u001b[37m  5210.00 ;13343,0,1617  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 4500: 771.16 Train Score: 518720.00 ;29037,0,10923 Loss: -0.002090 Valid Score: -22590.00 ;10706,0,4254 TEST Score: \u001b[47m \u001b[37m  60930.00 ;10829,0,4131  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 4800: 1618.26 Train Score: 0.00 ;0,0,0 Loss: -0.002916 Valid Score: -49210.00 ;6470,0,8490 TEST Score: \u001b[47m \u001b[37m  49530.00 ;6394,0,8566  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 5100: 1380.51 Train Score: 0.00 ;0,0,0 Loss: -0.003775 Valid Score: -26440.00 ;4681,0,10279 TEST Score: \u001b[47m \u001b[37m  63550.00 ;4723,0,10237  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 5400: 1891.56 Train Score: 0.00 ;0,0,0 Loss: -0.005003 Valid Score: -30570.00 ;5287,0,9673 TEST Score: \u001b[47m \u001b[37m  58310.00 ;5277,0,9683  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 5700: 1968.97 Train Score: 0.00 ;0,0,0 Loss: -0.006172 Valid Score: -9720.00 ;8860,0,6100 TEST Score: \u001b[47m \u001b[37m  44550.00 ;8927,0,6033  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 6000: 2269.43 Train Score: 561380.00 ;24956,0,15004 Loss: -0.007381 Valid Score: -14570.00 ;9119,0,5841 TEST Score: \u001b[47m \u001b[37m  56870.00 ;9166,0,5794  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 6300: 1921.42 Train Score: 0.00 ;0,0,0 Loss: -0.008287 Valid Score: -1050.00 ;7992,0,6968 TEST Score: \u001b[47m \u001b[37m  45740.00 ;7996,0,6964  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 6600: 2404.88 Train Score: 0.00 ;0,0,0 Loss: -0.009389 Valid Score: 2440.00 ;8597,0,6363 TEST Score: \u001b[47m \u001b[37m  46530.00 ;8635,0,6325  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 6900: 1662.62 Train Score: 0.00 ;0,0,0 Loss: -0.010183 Valid Score: -7240.00 ;8272,0,6688 TEST Score: \u001b[47m \u001b[37m  48240.00 ;8291,0,6669  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 7200: 3452.75 Train Score: 0.00 ;0,0,0 Loss: -0.011737 Valid Score: 1580.00 ;7539,0,7421 TEST Score: \u001b[47m \u001b[37m  56320.00 ;7511,0,7449  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 7500: 2849.93 Train Score: 494955.00 ;30691,0,9269 Loss: -0.013075 Valid Score: -2890.00 ;11322,0,3638 TEST Score: \u001b[47m \u001b[37m  5290.00 ;11377,0,3583  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 7800: 1261.95 Train Score: 0.00 ;0,0,0 Loss: -0.013398 Valid Score: -770.00 ;12020,0,2940 TEST Score: \u001b[47m \u001b[37m  -1460.00 ;12138,0,2822  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 8100: 1682.67 Train Score: 0.00 ;0,0,0 Loss: -0.013834 Valid Score: 3100.00 ;10479,0,4481 TEST Score: \u001b[47m \u001b[37m  6790.00 ;10566,0,4394  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 8400: 2558.30 Train Score: 0.00 ;0,0,0 Loss: -0.014683 Valid Score: 12140.00 ;7652,0,7308 TEST Score: \u001b[47m \u001b[37m  48150.00 ;7632,0,7328  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 8700: 2898.58 Train Score: 0.00 ;0,0,0 Loss: -0.015663 Valid Score: 9320.00 ;6137,0,8823 TEST Score: \u001b[47m \u001b[37m  60980.00 ;6088,0,8872  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 9000: 3004.23 Train Score: 761490.00 ;18362,0,21598 Loss: -0.016518 Valid Score: 2180.00 ;6794,0,8166 TEST Score: \u001b[47m \u001b[37m  67760.00 ;6768,0,8192  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 9300: 3202.98 Train Score: 0.00 ;0,0,0 Loss: -0.017286 Valid Score: 9420.00 ;8343,0,6617 TEST Score: \u001b[47m \u001b[37m  33660.00 ;8334,0,6626  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 9600: 2994.48 Train Score: 0.00 ;0,0,0 Loss: -0.018342 Valid Score: 20810.00 ;8977,0,5983 TEST Score: \u001b[47m \u001b[37m  32830.00 ;9048,0,5912  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 9900: 3049.40 Train Score: 0.00 ;0,0,0 Loss: -0.019127 Valid Score: 20010.00 ;7920,0,7040 TEST Score: \u001b[47m \u001b[37m  53840.00 ;7905,0,7055  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 10200: 3272.95 Train Score: 0.00 ;0,0,0 Loss: -0.019932 Valid Score: 33520.00 ;6537,0,8423 TEST Score: \u001b[47m \u001b[37m  56940.00 ;6521,0,8439  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 10500: 2929.57 Train Score: 835105.00 ;17202,0,22758 Loss: -0.020506 Valid Score: 28960.00 ;6325,0,8635 TEST Score: \u001b[47m \u001b[37m  63750.00 ;6321,0,8639  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 10800: 3244.17 Train Score: 0.00 ;0,0,0 Loss: -0.021264 Valid Score: 56120.00 ;7625,0,7335 TEST Score: \u001b[47m \u001b[37m  41880.00 ;7616,0,7344  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 11100: 3062.43 Train Score: 0.00 ;0,0,0 Loss: -0.021972 Valid Score: 43200.00 ;8313,0,6647 TEST Score: \u001b[47m \u001b[37m  46640.00 ;8301,0,6659  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 11400: 3053.62 Train Score: 0.00 ;0,0,0 Loss: -0.022591 Valid Score: 56290.00 ;7711,0,7249 TEST Score: \u001b[47m \u001b[37m  44640.00 ;7697,0,7263  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 11700: 3860.13 Train Score: 0.00 ;0,0,0 Loss: -0.023380 Valid Score: 37030.00 ;7149,0,7811 TEST Score: \u001b[47m \u001b[37m  48470.00 ;7171,0,7789  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 12000: 3270.12 Train Score: 842310.00 ;21070,0,18890 Loss: -0.024035 Valid Score: 40900.00 ;7763,0,7197 TEST Score: \u001b[47m \u001b[37m  43660.00 ;7756,0,7204  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 12300: 3959.10 Train Score: 0.00 ;0,0,0 Loss: -0.024888 Valid Score: 36910.00 ;8163,0,6797 TEST Score: \u001b[47m \u001b[37m  34390.00 ;8210,0,6750  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 12600: 2606.05 Train Score: 0.00 ;0,0,0 Loss: -0.025551 Valid Score: 30500.00 ;7882,0,7078 TEST Score: \u001b[47m \u001b[37m  33040.00 ;7941,0,7019  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 12900: 3181.60 Train Score: 0.00 ;0,0,0 Loss: -0.026043 Valid Score: 43660.00 ;7614,0,7346 TEST Score: \u001b[47m \u001b[37m  31770.00 ;7677,0,7283  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 13200: 3798.15 Train Score: 0.00 ;0,0,0 Loss: -0.026717 Valid Score: 46170.00 ;7770,0,7190 TEST Score: \u001b[47m \u001b[37m  37190.00 ;7844,0,7116  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 13500: 3562.10 Train Score: 987015.00 ;20354,0,19606 Loss: -0.027379 Valid Score: 41890.00 ;7454,0,7506 TEST Score: \u001b[47m \u001b[37m  41990.00 ;7531,0,7429  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 13800: 3581.38 Train Score: 0.00 ;0,0,0 Loss: -0.028124 Valid Score: 49020.00 ;7898,0,7062 TEST Score: \u001b[47m \u001b[37m  39090.00 ;7969,0,6991  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 14100: 4096.12 Train Score: 0.00 ;0,0,0 Loss: -0.028813 Valid Score: 42120.00 ;7880,0,7080 TEST Score: \u001b[47m \u001b[37m  30230.00 ;7977,0,6983  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 14400: 3659.73 Train Score: 0.00 ;0,0,0 Loss: -0.029435 Valid Score: 16590.00 ;7714,0,7246 TEST Score: \u001b[47m \u001b[37m  53330.00 ;7792,0,7168  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 14700: 3194.48 Train Score: 0.00 ;0,0,0 Loss: -0.029835 Valid Score: 30480.00 ;7181,0,7779 TEST Score: \u001b[47m \u001b[37m  39270.00 ;7261,0,7699  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 15000: 4518.43 Train Score: 1051630.00 ;21556,0,18404 Loss: -0.030594 Valid Score: 38050.00 ;7914,0,7046 TEST Score: \u001b[47m \u001b[37m  44230.00 ;7995,0,6965  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 15300: 4327.20 Train Score: 0.00 ;0,0,0 Loss: -0.031113 Valid Score: 19720.00 ;8741,0,6219 TEST Score: \u001b[47m \u001b[37m  19330.00 ;8864,0,6096  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 15600: 3629.00 Train Score: 0.00 ;0,0,0 Loss: -0.031456 Valid Score: 23350.00 ;7431,0,7529 TEST Score: \u001b[47m \u001b[37m  40970.00 ;7546,0,7414  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 15900: 3833.07 Train Score: 0.00 ;0,0,0 Loss: -0.031940 Valid Score: 23660.00 ;7415,0,7545 TEST Score: \u001b[47m \u001b[37m  40130.00 ;7526,0,7434  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 16200: 4361.05 Train Score: 0.00 ;0,0,0 Loss: -0.032618 Valid Score: 5930.00 ;8088,0,6872 TEST Score: \u001b[47m \u001b[37m  48820.00 ;8212,0,6748  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 16500: 3913.47 Train Score: 1152860.00 ;21355,0,18605 Loss: -0.033162 Valid Score: 18340.00 ;7839,0,7121 TEST Score: \u001b[47m \u001b[37m  44480.00 ;7935,0,7025  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 16800: 4135.05 Train Score: 0.00 ;0,0,0 Loss: -0.033579 Valid Score: 35490.00 ;7343,0,7617 TEST Score: \u001b[47m \u001b[37m  36590.00 ;7401,0,7559  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 17100: 5080.95 Train Score: 0.00 ;0,0,0 Loss: -0.034245 Valid Score: 22920.00 ;8776,0,6184 TEST Score: \u001b[47m \u001b[37m  10640.00 ;8916,0,6044  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 17400: 3907.35 Train Score: 0.00 ;0,0,0 Loss: -0.034730 Valid Score: 21920.00 ;7367,0,7593 TEST Score: \u001b[47m \u001b[37m  53800.00 ;7431,0,7529  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 17700: 4101.55 Train Score: 0.00 ;0,0,0 Loss: -0.035151 Valid Score: 26240.00 ;7513,0,7447 TEST Score: \u001b[47m \u001b[37m  64950.00 ;7613,0,7347  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 18000: 4288.22 Train Score: 1249660.00 ;22821,0,17139 Loss: -0.035571 Valid Score: 30250.00 ;8338,0,6622 TEST Score: \u001b[47m \u001b[37m  24830.00 ;8465,0,6495  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 18300: 4333.93 Train Score: 0.00 ;0,0,0 Loss: -0.035967 Valid Score: 26040.00 ;7094,0,7866 TEST Score: \u001b[47m \u001b[37m  40690.00 ;7153,0,7807  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 18600: 5278.38 Train Score: 0.00 ;0,0,0 Loss: -0.036541 Valid Score: 30460.00 ;7289,0,7671 TEST Score: \u001b[47m \u001b[37m  41970.00 ;7381,0,7579  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 18900: 5035.35 Train Score: 0.00 ;0,0,0 Loss: -0.037077 Valid Score: 18850.00 ;8340,0,6620 TEST Score: \u001b[47m \u001b[37m  14870.00 ;8470,0,6490  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 19200: 4384.00 Train Score: 0.00 ;0,0,0 Loss: -0.037502 Valid Score: 32930.00 ;7191,0,7769 TEST Score: \u001b[47m \u001b[37m  43900.00 ;7291,0,7669  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 19500: 5189.42 Train Score: 1323695.00 ;21227,0,18733 Loss: -0.038002 Valid Score: 22330.00 ;7767,0,7193 TEST Score: \u001b[47m \u001b[37m  36160.00 ;7898,0,7062  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 19800: 5215.78 Train Score: 0.00 ;0,0,0 Loss: -0.038558 Valid Score: 22320.00 ;8316,0,6644 TEST Score: \u001b[47m \u001b[37m  11410.00 ;8429,0,6531  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 20100: 4823.37 Train Score: 0.00 ;0,0,0 Loss: -0.038981 Valid Score: 45350.00 ;7399,0,7561 TEST Score: \u001b[47m \u001b[37m  43700.00 ;7533,0,7427  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 20400: 4610.02 Train Score: 0.00 ;0,0,0 Loss: -0.039406 Valid Score: 25720.00 ;8187,0,6773 TEST Score: \u001b[47m \u001b[37m  36640.00 ;8313,0,6647  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 20700: 4759.98 Train Score: 0.00 ;0,0,0 Loss: -0.039805 Valid Score: 30550.00 ;6888,0,8072 TEST Score: \u001b[47m \u001b[37m  42890.00 ;6972,0,7988  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 21000: 5329.17 Train Score: 1372280.00 ;22104,0,17856 Loss: -0.040343 Valid Score: 34830.00 ;8083,0,6877 TEST Score: \u001b[47m \u001b[37m  25590.00 ;8230,0,6730  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 21300: 4298.60 Train Score: 0.00 ;0,0,0 Loss: -0.040607 Valid Score: 59010.00 ;7697,0,7263 TEST Score: \u001b[47m \u001b[37m  26630.00 ;7830,0,7130  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 21600: 5827.57 Train Score: 0.00 ;0,0,0 Loss: -0.041197 Valid Score: 55820.00 ;7302,0,7658 TEST Score: \u001b[47m \u001b[37m  40760.00 ;7444,0,7516  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 21900: 5339.40 Train Score: 0.00 ;0,0,0 Loss: -0.041623 Valid Score: 41140.00 ;7959,0,7001 TEST Score: \u001b[47m \u001b[37m  28210.00 ;8083,0,6877  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 22200: 4538.05 Train Score: 0.00 ;0,0,0 Loss: -0.041968 Valid Score: 35650.00 ;7009,0,7951 TEST Score: \u001b[47m \u001b[37m  43220.00 ;7107,0,7853  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 22500: 5727.57 Train Score: 1457815.00 ;22212,0,17748 Loss: -0.042524 Valid Score: 54170.00 ;8100,0,6860 TEST Score: \u001b[47m \u001b[37m  30570.00 ;8260,0,6700  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 22800: 6245.18 Train Score: 0.00 ;0,0,0 Loss: -0.043109 Valid Score: 19660.00 ;6652,0,8308 TEST Score: \u001b[47m \u001b[37m  31300.00 ;6724,0,8236  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 23100: 4774.70 Train Score: 0.00 ;0,0,0 Loss: -0.043457 Valid Score: 27380.00 ;8867,0,6093 TEST Score: \u001b[47m \u001b[37m  2250.00 ;9043,0,5917  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 23400: 5670.63 Train Score: 0.00 ;0,0,0 Loss: -0.043983 Valid Score: -2060.00 ;6162,0,8798 TEST Score: \u001b[47m \u001b[37m  36340.00 ;6171,0,8789  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 23700: 4728.20 Train Score: 0.00 ;0,0,0 Loss: -0.044242 Valid Score: 36870.00 ;7465,0,7495 TEST Score: \u001b[47m \u001b[37m  19400.00 ;7668,0,7292  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 24000: 5300.62 Train Score: 1518655.00 ;21737,0,18223 Loss: -0.044820 Valid Score: 35670.00 ;7924,0,7036 TEST Score: \u001b[47m \u001b[37m  16760.00 ;8124,0,6836  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 24300: 6589.03 Train Score: 0.00 ;0,0,0 Loss: -0.045502 Valid Score: 43140.00 ;7345,0,7615 TEST Score: \u001b[47m \u001b[37m  35950.00 ;7488,0,7472  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 24600: 6064.45 Train Score: 0.00 ;0,0,0 Loss: -0.045986 Valid Score: 31110.00 ;7661,0,7299 TEST Score: \u001b[47m \u001b[37m  37260.00 ;7799,0,7161  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 24900: 6695.47 Train Score: 0.00 ;0,0,0 Loss: -0.046600 Valid Score: 38210.00 ;7327,0,7633 TEST Score: \u001b[47m \u001b[37m  41090.00 ;7466,0,7494  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 25200: 6467.25 Train Score: 0.00 ;0,0,0 Loss: -0.047130 Valid Score: 37940.00 ;7523,0,7437 TEST Score: \u001b[47m \u001b[37m  36150.00 ;7689,0,7271  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 25500: 5814.60 Train Score: 1704265.00 ;19715,0,20245 Loss: -0.047769 Valid Score: 39010.00 ;7245,0,7715 TEST Score: \u001b[47m \u001b[37m  28790.00 ;7385,0,7575  \u001b[0m \n",
      "done\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "Average reward for training episode 25800: 6937.60 Train Score: 0.00 ;0,0,0 Loss: -0.048350 Valid Score: 32230.00 ;7361,0,7599 TEST Score: \u001b[47m \u001b[37m  26060.00 ;7463,0,7497  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 26100: 5386.53 Train Score: 0.00 ;0,0,0 Loss: -0.048840 Valid Score: 34530.00 ;7680,0,7280 TEST Score: \u001b[47m \u001b[37m  8430.00 ;7849,0,7111  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 26400: 6110.48 Train Score: 0.00 ;0,0,0 Loss: -0.049436 Valid Score: 33920.00 ;7539,0,7421 TEST Score: \u001b[47m \u001b[37m  9040.00 ;7708,0,7252  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 26700: 6843.15 Train Score: 0.00 ;0,0,0 Loss: -0.050102 Valid Score: 21300.00 ;7778,0,7182 TEST Score: \u001b[47m \u001b[37m  23230.00 ;7964,0,6996  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 27000: 5870.86 Train Score: 1704580.00 ;18510,0,21450 Loss: -0.050487 Valid Score: -711.00 ;6810,1,8149 TEST Score: \u001b[47m \u001b[37m  29380.00 ;6848,0,8112  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 27300: 6765.59 Train Score: 0.00 ;0,0,0 Loss: -0.051061 Valid Score: 16936.00 ;7531,4,7425 TEST Score: \u001b[47m \u001b[37m  39017.00 ;7664,3,7293  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 27600: 5989.47 Train Score: 0.00 ;0,0,0 Loss: -0.051437 Valid Score: 29353.00 ;7553,7,7400 TEST Score: \u001b[47m \u001b[37m  21844.00 ;7696,6,7258  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 27900: 7182.24 Train Score: 0.00 ;0,0,0 Loss: -0.051952 Valid Score: -7494.00 ;6626,14,8320 TEST Score: \u001b[47m \u001b[37m  21089.00 ;6743,11,8206  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 28200: 6877.77 Train Score: 0.00 ;0,0,0 Loss: -0.052551 Valid Score: -1559.00 ;7117,9,7834 TEST Score: \u001b[47m \u001b[37m  17924.00 ;7217,6,7737  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 28500: 6565.95 Train Score: 1648784.00 ;24244,1,15715 Loss: -0.052968 Valid Score: 42215.00 ;8855,5,6100 TEST Score: \u001b[47m \u001b[37m  17338.00 ;9085,2,5873  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 28800: 5632.68 Train Score: 0.00 ;0,0,0 Loss: -0.053244 Valid Score: -39750.00 ;6430,0,8530 TEST Score: \u001b[47m \u001b[37m  36700.00 ;6497,0,8463  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 29100: 6429.12 Train Score: 0.00 ;0,0,0 Loss: -0.053719 Valid Score: 19048.00 ;8668,2,6290 TEST Score: \u001b[47m \u001b[37m  -4460.00 ;8868,0,6092  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 29400: 6210.13 Train Score: 0.00 ;0,0,0 Loss: -0.054092 Valid Score: -28730.00 ;6066,0,8894 TEST Score: \u001b[47m \u001b[37m  49250.00 ;6120,0,8840  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 29700: 5856.67 Train Score: 0.00 ;0,0,0 Loss: -0.054348 Valid Score: 9790.00 ;8417,0,6543 TEST Score: \u001b[47m \u001b[37m  3210.00 ;8601,0,6359  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 30000: 6465.85 Train Score: 1903475.00 ;18834,0,21126 Loss: -0.054701 Valid Score: 4390.00 ;6893,0,8067 TEST Score: \u001b[47m \u001b[37m  36830.00 ;6983,0,7977  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 30300: 7374.25 Train Score: 0.00 ;0,0,0 Loss: -0.055779 Valid Score: 7700.00 ;7493,0,7467 TEST Score: \u001b[47m \u001b[37m  45100.00 ;7648,0,7312  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 30600: 6749.97 Train Score: 0.00 ;0,0,0 Loss: -0.056942 Valid Score: 8710.00 ;7487,0,7473 TEST Score: \u001b[47m \u001b[37m  52750.00 ;7635,0,7325  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 30900: 7871.12 Train Score: 0.00 ;0,0,0 Loss: -0.058305 Valid Score: -1800.00 ;7214,0,7746 TEST Score: \u001b[47m \u001b[37m  39270.00 ;7320,0,7640  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 31200: 8255.57 Train Score: 0.00 ;0,0,0 Loss: -0.059527 Valid Score: -1660.00 ;7495,0,7465 TEST Score: \u001b[47m \u001b[37m  27520.00 ;7611,0,7349  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 31500: 7960.15 Train Score: 2017595.00 ;20159,0,19801 Loss: -0.060687 Valid Score: -14020.00 ;7373,0,7587 TEST Score: \u001b[47m \u001b[37m  41320.00 ;7522,0,7438  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 31800: 7186.27 Train Score: 0.00 ;0,0,0 Loss: -0.061854 Valid Score: 11630.00 ;7501,0,7459 TEST Score: \u001b[47m \u001b[37m  30910.00 ;7624,0,7336  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 32100: 8259.08 Train Score: 0.00 ;0,0,0 Loss: -0.063010 Valid Score: 4370.00 ;7395,0,7565 TEST Score: \u001b[47m \u001b[37m  16380.00 ;7485,0,7475  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 32400: 6586.72 Train Score: 0.00 ;0,0,0 Loss: -0.064161 Valid Score: -2321.00 ;7200,1,7759 TEST Score: \u001b[47m \u001b[37m  39880.00 ;7339,0,7621  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 32700: 7041.73 Train Score: 0.00 ;0,0,0 Loss: -0.065380 Valid Score: 23089.00 ;8138,1,6821 TEST Score: \u001b[47m \u001b[37m  19590.00 ;8347,0,6613  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 33000: 7204.95 Train Score: 2061235.00 ;18454,0,21506 Loss: -0.066408 Valid Score: -9930.00 ;6735,0,8225 TEST Score: \u001b[47m \u001b[37m  41740.00 ;6878,0,8082  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 33300: 8303.50 Train Score: 0.00 ;0,0,0 Loss: -0.067524 Valid Score: -3420.00 ;6936,0,8024 TEST Score: \u001b[47m \u001b[37m  20460.00 ;7042,0,7918  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 33600: 7339.22 Train Score: 0.00 ;0,0,0 Loss: -0.068562 Valid Score: 19599.00 ;8044,1,6915 TEST Score: \u001b[47m \u001b[37m  7699.00 ;8247,1,6712  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 33900: 7043.35 Train Score: 0.00 ;0,0,0 Loss: -0.069587 Valid Score: -9192.00 ;7415,2,7543 TEST Score: \u001b[47m \u001b[37m  37789.00 ;7559,1,7400  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 34200: 7857.70 Train Score: 0.00 ;0,0,0 Loss: -0.070684 Valid Score: -17312.00 ;7091,2,7867 TEST Score: \u001b[47m \u001b[37m  29087.00 ;7234,3,7723  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 34500: 7953.33 Train Score: 2086201.00 ;22385,4,17571 Loss: -0.071719 Valid Score: 12316.00 ;8216,4,6740 TEST Score: \u001b[47m \u001b[37m  11188.00 ;8402,2,6556  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 34800: 7613.77 Train Score: 0.00 ;0,0,0 Loss: -0.072643 Valid Score: -3121.00 ;6691,1,8268 TEST Score: \u001b[47m \u001b[37m  21959.00 ;6769,1,8190  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 35100: 8615.73 Train Score: 0.00 ;0,0,0 Loss: -0.073674 Valid Score: 30328.00 ;7859,2,7099 TEST Score: \u001b[47m \u001b[37m  9468.00 ;8028,2,6930  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 35400: 8596.20 Train Score: 0.00 ;0,0,0 Loss: -0.074653 Valid Score: 19736.00 ;8063,4,6893 TEST Score: \u001b[47m \u001b[37m  12918.00 ;8239,2,6719  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 35700: 8879.98 Train Score: 0.00 ;0,0,0 Loss: -0.075658 Valid Score: 898.00 ;6881,2,8077 TEST Score: \u001b[47m \u001b[37m  22820.00 ;7011,0,7949  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 36000: 8703.62 Train Score: 2253909.00 ;19588,1,20371 Loss: -0.076853 Valid Score: -4041.00 ;7142,1,7817 TEST Score: \u001b[47m \u001b[37m  21810.00 ;7303,0,7657  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 36300: 7799.73 Train Score: 0.00 ;0,0,0 Loss: -0.077881 Valid Score: 25129.00 ;7823,1,7136 TEST Score: \u001b[47m \u001b[37m  14360.00 ;7995,0,6965  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 36600: 8615.90 Train Score: 0.00 ;0,0,0 Loss: -0.078777 Valid Score: 4980.00 ;6774,0,8186 TEST Score: \u001b[47m \u001b[37m  32980.00 ;6908,0,8052  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 36900: 8559.46 Train Score: 0.00 ;0,0,0 Loss: -0.079663 Valid Score: -14151.00 ;7332,1,7627 TEST Score: \u001b[47m \u001b[37m  28200.00 ;7470,0,7490  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 37200: 7644.02 Train Score: 0.00 ;0,0,0 Loss: -0.080266 Valid Score: 11148.00 ;8192,2,6766 TEST Score: \u001b[47m \u001b[37m  819.00 ;8371,1,6588  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 37500: 8640.80 Train Score: 2272339.00 ;21780,1,18179 Loss: -0.081333 Valid Score: 8959.00 ;8009,1,6950 TEST Score: \u001b[47m \u001b[37m  17800.00 ;8179,0,6781  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 37800: 7884.40 Train Score: 0.00 ;0,0,0 Loss: -0.082179 Valid Score: 879.00 ;6712,1,8247 TEST Score: \u001b[47m \u001b[37m  27409.00 ;6847,1,8112  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 38100: 8191.19 Train Score: 0.00 ;0,0,0 Loss: -0.083266 Valid Score: 5888.00 ;7080,2,7878 TEST Score: \u001b[47m \u001b[37m  23259.00 ;7266,1,7693  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 38400: 8245.84 Train Score: 0.00 ;0,0,0 Loss: -0.084280 Valid Score: 18597.00 ;7776,3,7181 TEST Score: \u001b[47m \u001b[37m  22899.00 ;7926,1,7033  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 38700: 8573.23 Train Score: 0.00 ;0,0,0 Loss: -0.085103 Valid Score: 10708.00 ;8562,2,6396 TEST Score: \u001b[47m \u001b[37m  -3842.00 ;8763,2,6195  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 39000: 9646.32 Train Score: 2309551.00 ;21433,4,18523 Loss: -0.086108 Valid Score: -3504.00 ;7824,4,7132 TEST Score: \u001b[47m \u001b[37m  28678.00 ;7997,2,6961  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 39300: 9381.02 Train Score: 0.00 ;0,0,0 Loss: -0.087241 Valid Score: -1874.00 ;7907,4,7049 TEST Score: \u001b[47m \u001b[37m  14037.00 ;8095,3,6862  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 39600: 8582.59 Train Score: 0.00 ;0,0,0 Loss: -0.088119 Valid Score: 7196.00 ;6975,4,7981 TEST Score: \u001b[47m \u001b[37m  26558.00 ;7110,2,7848  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 39900: 8542.68 Train Score: 0.00 ;0,0,0 Loss: -0.088900 Valid Score: 5009.00 ;7234,1,7725 TEST Score: \u001b[47m \u001b[37m  32529.00 ;7366,1,7593  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 40200: 8325.85 Train Score: 0.00 ;0,0,0 Loss: -0.089601 Valid Score: 6139.00 ;8139,1,6820 TEST Score: \u001b[47m \u001b[37m  10980.00 ;8288,0,6672  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 40500: 9552.68 Train Score: 2437709.00 ;20685,1,19274 Loss: -0.090536 Valid Score: 21469.00 ;7579,1,7380 TEST Score: \u001b[47m \u001b[37m  25850.00 ;7754,0,7206  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 40800: 10579.67 Train Score: 0.00 ;0,0,0 Loss: -0.091580 Valid Score: 11379.00 ;7276,1,7683 TEST Score: \u001b[47m \u001b[37m  20760.00 ;7423,0,7537  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 41100: 8601.77 Train Score: 0.00 ;0,0,0 Loss: -0.092352 Valid Score: 3029.00 ;6809,1,8150 TEST Score: \u001b[47m \u001b[37m  37440.00 ;6912,0,8048  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 41400: 8869.52 Train Score: 0.00 ;0,0,0 Loss: -0.093104 Valid Score: 1929.00 ;7163,1,7796 TEST Score: \u001b[47m \u001b[37m  19330.00 ;7293,0,7667  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 41700: 9102.78 Train Score: 0.00 ;0,0,0 Loss: -0.093798 Valid Score: 11699.00 ;7681,1,7278 TEST Score: \u001b[47m \u001b[37m  21190.00 ;7862,0,7098  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 42000: 9650.85 Train Score: 2420383.00 ;20935,2,19023 Loss: -0.094742 Valid Score: 15559.00 ;7691,1,7268 TEST Score: \u001b[47m \u001b[37m  19398.00 ;7843,2,7115  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 42300: 8524.67 Train Score: 0.00 ;0,0,0 Loss: -0.095301 Valid Score: 2666.00 ;7758,4,7198 TEST Score: \u001b[47m \u001b[37m  12169.00 ;7909,1,7050  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 42600: 9032.52 Train Score: 0.00 ;0,0,0 Loss: -0.096208 Valid Score: 16457.00 ;7760,3,7197 TEST Score: \u001b[47m \u001b[37m  27057.00 ;7908,3,7049  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 42900: 8711.86 Train Score: 0.00 ;0,0,0 Loss: -0.097213 Valid Score: 16037.00 ;7530,3,7427 TEST Score: \u001b[47m \u001b[37m  16177.00 ;7705,3,7252  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 43200: 9500.42 Train Score: 0.00 ;0,0,0 Loss: -0.098026 Valid Score: 7257.00 ;7506,3,7451 TEST Score: \u001b[47m \u001b[37m  22327.00 ;7678,3,7279  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 43500: 8943.06 Train Score: 2493047.00 ;20260,3,19697 Loss: -0.098770 Valid Score: 3208.00 ;7405,2,7553 TEST Score: \u001b[47m \u001b[37m  17408.00 ;7546,2,7412  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 43800: 8886.94 Train Score: 0.00 ;0,0,0 Loss: -0.099462 Valid Score: 12939.00 ;7535,1,7424 TEST Score: \u001b[47m \u001b[37m  16888.00 ;7664,2,7294  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 44100: 9326.51 Train Score: 0.00 ;0,0,0 Loss: -0.100158 Valid Score: -2272.00 ;7825,2,7133 TEST Score: \u001b[47m \u001b[37m  24969.00 ;7964,1,6995  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 44400: 8457.11 Train Score: 0.00 ;0,0,0 Loss: -0.100771 Valid Score: -6083.00 ;7954,3,7003 TEST Score: \u001b[47m \u001b[37m  13789.00 ;8107,1,6852  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 44700: 8575.47 Train Score: 0.00 ;0,0,0 Loss: -0.101478 Valid Score: 3798.00 ;6942,2,8016 TEST Score: \u001b[47m \u001b[37m  1338.00 ;7069,2,7889  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 45000: 9953.01 Train Score: 2239321.00 ;17677,4,22279 Loss: -0.102212 Valid Score: -1093.00 ;6484,3,8473 TEST Score: \u001b[47m \u001b[37m  2328.00 ;6572,2,8386  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 45300: 7779.14 Train Score: 0.00 ;0,0,0 Loss: -0.102919 Valid Score: -14806.00 ;8239,6,6715 TEST Score: \u001b[47m \u001b[37m  7034.00 ;8425,6,6529  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 45600: 6048.93 Train Score: 0.00 ;0,0,0 Loss: -0.103300 Valid Score: 10556.00 ;8403,14,6543 TEST Score: \u001b[47m \u001b[37m  -6466.00 ;8594,6,6360  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 45900: 9202.90 Train Score: 0.00 ;0,0,0 Loss: -0.104050 Valid Score: -2694.00 ;6341,4,8615 TEST Score: \u001b[47m \u001b[37m  1838.00 ;6431,2,8527  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 46200: 8305.38 Train Score: 0.00 ;0,0,0 Loss: -0.104718 Valid Score: -2845.00 ;7029,5,7926 TEST Score: \u001b[47m \u001b[37m  22186.00 ;7184,4,7772  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 46500: 9339.56 Train Score: 2498725.00 ;21074,5,18881 Loss: -0.105482 Valid Score: -4334.00 ;7722,4,7234 TEST Score: \u001b[47m \u001b[37m  17627.00 ;7868,3,7089  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 46800: 9764.24 Train Score: 0.00 ;0,0,0 Loss: -0.106297 Valid Score: -17792.00 ;7802,2,7156 TEST Score: \u001b[47m \u001b[37m  15739.00 ;7969,1,6990  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 47100: 9847.57 Train Score: 0.00 ;0,0,0 Loss: -0.106907 Valid Score: -11280.00 ;8150,0,6810 TEST Score: \u001b[47m \u001b[37m  7450.00 ;8325,0,6635  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 47400: 8140.62 Train Score: 0.00 ;0,0,0 Loss: -0.107580 Valid Score: 1820.00 ;7595,0,7365 TEST Score: \u001b[47m \u001b[37m  12020.00 ;7766,0,7194  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 47700: 9100.41 Train Score: 0.00 ;0,0,0 Loss: -0.108204 Valid Score: -8980.00 ;6207,0,8753 TEST Score: \u001b[47m \u001b[37m  -2030.00 ;6284,0,8676  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 48000: 7741.68 Train Score: 2506294.00 ;19094,1,20865 Loss: -0.108859 Valid Score: 720.00 ;6973,0,7987 TEST Score: \u001b[47m \u001b[37m  5550.00 ;7092,0,7868  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 48300: 9117.78 Train Score: 0.00 ;0,0,0 Loss: -0.109539 Valid Score: 10630.00 ;7945,0,7015 TEST Score: \u001b[47m \u001b[37m  -2870.00 ;8108,0,6852  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 48600: 9593.33 Train Score: 0.00 ;0,0,0 Loss: -0.110303 Valid Score: -6050.00 ;8533,0,6427 TEST Score: \u001b[47m \u001b[37m  -9880.00 ;8724,0,6236  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 48900: 9254.29 Train Score: 0.00 ;0,0,0 Loss: -0.110879 Valid Score: 5540.00 ;7528,0,7432 TEST Score: \u001b[47m \u001b[37m  16360.00 ;7679,0,7281  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 49200: 8164.01 Train Score: 0.00 ;0,0,0 Loss: -0.111506 Valid Score: 17900.00 ;6741,0,8219 TEST Score: \u001b[47m \u001b[37m  7080.00 ;6876,0,8084  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 49500: 8982.55 Train Score: 2643859.00 ;21148,1,18811 Loss: -0.112295 Valid Score: 1540.00 ;7766,0,7194 TEST Score: \u001b[47m \u001b[37m  17900.00 ;7946,0,7014  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 49800: 9684.17 Train Score: 0.00 ;0,0,0 Loss: -0.113188 Valid Score: -5591.00 ;8296,1,6663 TEST Score: \u001b[47m \u001b[37m  -6340.00 ;8454,0,6506  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 50100: 11058.53 Train Score: 0.00 ;0,0,0 Loss: -0.114102 Valid Score: -9452.00 ;8070,2,6888 TEST Score: \u001b[47m \u001b[37m  1850.00 ;8213,0,6747  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 50400: 9964.33 Train Score: 0.00 ;0,0,0 Loss: -0.115030 Valid Score: -6222.00 ;7017,2,7941 TEST Score: \u001b[47m \u001b[37m  2700.00 ;7137,0,7823  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 50700: 8506.68 Train Score: 0.00 ;0,0,0 Loss: -0.115647 Valid Score: -11492.00 ;7416,2,7542 TEST Score: \u001b[47m \u001b[37m  19708.00 ;7576,2,7382  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 51000: 9897.47 Train Score: 2655311.00 ;21087,4,18869 Loss: -0.116390 Valid Score: -5792.00 ;7726,2,7232 TEST Score: \u001b[47m \u001b[37m  22078.00 ;7891,2,7067  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 51300: 10515.02 Train Score: 0.00 ;0,0,0 Loss: -0.117308 Valid Score: -1092.00 ;7661,2,7297 TEST Score: \u001b[47m \u001b[37m  19368.00 ;7837,2,7121  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 51600: 10178.97 Train Score: 0.00 ;0,0,0 Loss: -0.117970 Valid Score: 457.00 ;7645,3,7312 TEST Score: \u001b[47m \u001b[37m  22298.00 ;7808,2,7150  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 51900: 9600.94 Train Score: 0.00 ;0,0,0 Loss: -0.118816 Valid Score: -3773.00 ;7517,3,7440 TEST Score: \u001b[47m \u001b[37m  9778.00 ;7667,2,7291  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 52200: 9951.43 Train Score: 0.00 ;0,0,0 Loss: -0.119566 Valid Score: -4732.00 ;7656,2,7302 TEST Score: \u001b[47m \u001b[37m  17089.00 ;7771,1,7188  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 52500: 9027.93 Train Score: 2283469.00 ;17555,1,22404 Loss: -0.120270 Valid Score: 7290.00 ;6448,0,8512 TEST Score: \u001b[47m \u001b[37m  -20941.00 ;6546,1,8413  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 52800: 8917.60 Train Score: 0.00 ;0,0,0 Loss: -0.120855 Valid Score: -18940.00 ;5588,0,9372 TEST Score: \u001b[47m \u001b[37m  -7121.00 ;5671,1,9288  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 53100: 5623.31 Train Score: 0.00 ;0,0,0 Loss: -0.121099 Valid Score: 15888.00 ;9848,2,5110 TEST Score: \u001b[47m \u001b[37m  -16291.00 ;10046,1,4913  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 53400: 6286.37 Train Score: 0.00 ;0,0,0 Loss: -0.121274 Valid Score: -12522.00 ;5613,2,9345 TEST Score: \u001b[47m \u001b[37m  37679.00 ;5655,1,9304  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 53700: 5522.25 Train Score: 0.00 ;0,0,0 Loss: -0.121612 Valid Score: -2300.00 ;10811,0,4149 TEST Score: \u001b[47m \u001b[37m  254.00 ;11029,6,3925  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 54000: 5460.72 Train Score: 1520725.00 ;14640,5,25315 Loss: -0.121571 Valid Score: -3893.00 ;5472,3,9485 TEST Score: \u001b[47m \u001b[37m  36256.00 ;5421,4,9535  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 54300: 6050.67 Train Score: 0.00 ;0,0,0 Loss: -0.121509 Valid Score: -3492.00 ;8614,2,6344 TEST Score: \u001b[47m \u001b[37m  9627.00 ;8772,3,6185  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 54600: 8435.20 Train Score: 0.00 ;0,0,0 Loss: -0.121893 Valid Score: -3983.00 ;7004,3,7953 TEST Score: \u001b[47m \u001b[37m  626.00 ;7132,4,7824  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 54900: 6634.37 Train Score: 0.00 ;0,0,0 Loss: -0.121959 Valid Score: 638.00 ;7782,2,7176 TEST Score: \u001b[47m \u001b[37m  4578.00 ;7945,2,7013  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 55200: 9658.39 Train Score: 0.00 ;0,0,0 Loss: -0.122501 Valid Score: 10038.00 ;7552,2,7406 TEST Score: \u001b[47m \u001b[37m  -12502.00 ;7710,2,7248  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 55500: 8036.71 Train Score: 2511596.00 ;20492,4,19464 Loss: -0.122697 Valid Score: 7078.00 ;7512,2,7446 TEST Score: \u001b[47m \u001b[37m  -22922.00 ;7657,2,7301  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 55800: 9146.89 Train Score: 0.00 ;0,0,0 Loss: -0.123177 Valid Score: 4336.00 ;6990,4,7966 TEST Score: \u001b[47m \u001b[37m  -10502.00 ;7120,2,7838  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 56100: 9005.96 Train Score: 0.00 ;0,0,0 Loss: -0.123665 Valid Score: 609.00 ;8442,1,6517 TEST Score: \u001b[47m \u001b[37m  863.00 ;8571,7,6382  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 56400: 8885.23 Train Score: 0.00 ;0,0,0 Loss: -0.124118 Valid Score: -8933.00 ;7657,3,7300 TEST Score: \u001b[47m \u001b[37m  5578.00 ;7817,2,7141  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 56700: 9875.32 Train Score: 0.00 ;0,0,0 Loss: -0.124419 Valid Score: -1562.00 ;6765,2,8193 TEST Score: \u001b[47m \u001b[37m  -7621.00 ;6941,1,8018  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 57000: 10563.38 Train Score: 2646661.00 ;21706,4,18250 Loss: -0.125080 Valid Score: 5848.00 ;8004,2,6954 TEST Score: \u001b[47m \u001b[37m  5468.00 ;8172,2,6786  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 57300: 10316.02 Train Score: 0.00 ;0,0,0 Loss: -0.125511 Valid Score: 8758.00 ;7582,2,7376 TEST Score: \u001b[47m \u001b[37m  6128.00 ;7706,2,7252  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 57600: 9314.08 Train Score: 0.00 ;0,0,0 Loss: -0.126312 Valid Score: 5688.00 ;7611,2,7347 TEST Score: \u001b[47m \u001b[37m  8258.00 ;7787,2,7171  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 57900: 9286.35 Train Score: 0.00 ;0,0,0 Loss: -0.126869 Valid Score: -2452.00 ;7336,2,7622 TEST Score: \u001b[47m \u001b[37m  20089.00 ;7531,1,7428  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 58200: 10056.19 Train Score: 0.00 ;0,0,0 Loss: -0.127232 Valid Score: -6362.00 ;7232,2,7726 TEST Score: \u001b[47m \u001b[37m  12049.00 ;7389,1,7570  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 58500: 9528.85 Train Score: 2560891.00 ;22297,4,17659 Loss: -0.127656 Valid Score: 2968.00 ;8234,2,6724 TEST Score: \u001b[47m \u001b[37m  4828.00 ;8358,2,6600  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 58800: 10317.23 Train Score: 0.00 ;0,0,0 Loss: -0.128327 Valid Score: 6178.00 ;7552,2,7406 TEST Score: \u001b[47m \u001b[37m  7398.00 ;7676,2,7282  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 59100: 10223.95 Train Score: 0.00 ;0,0,0 Loss: -0.128718 Valid Score: 2578.00 ;6956,2,8002 TEST Score: \u001b[47m \u001b[37m  -6861.00 ;7105,1,7854  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 59400: 10085.71 Train Score: 0.00 ;0,0,0 Loss: -0.129497 Valid Score: 2569.00 ;6386,1,8573 TEST Score: \u001b[47m \u001b[37m  -14561.00 ;6480,1,8479  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 59700: 9218.13 Train Score: 0.00 ;0,0,0 Loss: -0.130061 Valid Score: 5630.00 ;8592,0,6368 TEST Score: \u001b[47m \u001b[37m  -7991.00 ;8790,1,6169  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 60000: 7357.09 Train Score: 2645639.00 ;21335,6,18619 Loss: -0.130336 Valid Score: 4448.00 ;7879,2,7079 TEST Score: \u001b[47m \u001b[37m  -2892.00 ;8018,2,6940  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 60300: 8796.13 Train Score: 0.00 ;0,0,0 Loss: -0.130731 Valid Score: -2991.00 ;6559,1,8400 TEST Score: \u001b[47m \u001b[37m  15468.00 ;6663,2,8295  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 60600: 10238.26 Train Score: 0.00 ;0,0,0 Loss: -0.131019 Valid Score: 3158.00 ;8209,2,6749 TEST Score: \u001b[47m \u001b[37m  7138.00 ;8386,2,6572  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 60900: 10778.55 Train Score: 0.00 ;0,0,0 Loss: -0.131475 Valid Score: 5939.00 ;8332,1,6627 TEST Score: \u001b[47m \u001b[37m  -292.00 ;8483,2,6475  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 61200: 8703.01 Train Score: 0.00 ;0,0,0 Loss: -0.131815 Valid Score: -1812.00 ;8419,2,6539 TEST Score: \u001b[47m \u001b[37m  2938.00 ;8530,2,6428  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 61500: 9135.31 Train Score: 2137058.00 ;16942,2,23016 Loss: -0.132270 Valid Score: -13641.00 ;6234,1,8725 TEST Score: \u001b[47m \u001b[37m  -10432.00 ;6259,2,8699  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 61800: 7656.31 Train Score: 0.00 ;0,0,0 Loss: -0.132521 Valid Score: -1622.00 ;10128,2,4830 TEST Score: \u001b[47m \u001b[37m  -2091.00 ;10345,1,4614  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 62100: 5362.95 Train Score: 0.00 ;0,0,0 Loss: -0.132305 Valid Score: -26261.00 ;4169,1,10790 TEST Score: \u001b[47m \u001b[37m  33830.00 ;4174,0,10786  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 62400: 3689.10 Train Score: 0.00 ;0,0,0 Loss: -0.131668 Valid Score: -20710.00 ;3202,0,11758 TEST Score: \u001b[47m \u001b[37m  34020.00 ;3199,0,11761  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 62700: 2792.45 Train Score: 0.00 ;0,0,0 Loss: -0.130884 Valid Score: 1480.00 ;5509,0,9451 TEST Score: \u001b[47m \u001b[37m  19240.00 ;5610,0,9350  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 63000: 6446.85 Train Score: 1172185.00 ;29084,0,10876 Loss: -0.130747 Valid Score: 15160.00 ;10673,0,4287 TEST Score: \u001b[47m \u001b[37m  -18030.00 ;10827,0,4133  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 63300: 4336.80 Train Score: 0.00 ;0,0,0 Loss: -0.130210 Valid Score: 23920.00 ;10159,0,4801 TEST Score: \u001b[47m \u001b[37m  -28660.00 ;10330,0,4630  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 63600: 7611.03 Train Score: 0.00 ;0,0,0 Loss: -0.130239 Valid Score: 36750.00 ;6906,0,8054 TEST Score: \u001b[47m \u001b[37m  33230.00 ;7047,0,7913  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 63900: 6025.10 Train Score: 0.00 ;0,0,0 Loss: -0.129985 Valid Score: -15980.00 ;5220,0,9740 TEST Score: \u001b[47m \u001b[37m  47930.00 ;5261,0,9699  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 64200: 5815.81 Train Score: 0.00 ;0,0,0 Loss: -0.129607 Valid Score: 25880.00 ;8029,0,6931 TEST Score: \u001b[47m \u001b[37m  29429.00 ;8163,1,6796  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 64500: 7075.42 Train Score: 1452059.00 ;26647,1,13312 Loss: -0.129540 Valid Score: 26090.00 ;9804,0,5156 TEST Score: \u001b[47m \u001b[37m  -22821.00 ;10021,1,4938  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 64800: 6629.43 Train Score: 0.00 ;0,0,0 Loss: -0.129540 Valid Score: 8579.00 ;6384,1,8575 TEST Score: \u001b[47m \u001b[37m  29659.00 ;6378,1,8581  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 65100: 6773.16 Train Score: 0.00 ;0,0,0 Loss: -0.129421 Valid Score: 4650.00 ;6729,0,8231 TEST Score: \u001b[47m \u001b[37m  15819.00 ;6748,1,8211  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 65400: 7765.88 Train Score: 0.00 ;0,0,0 Loss: -0.129350 Valid Score: 4700.00 ;8722,0,6238 TEST Score: \u001b[47m \u001b[37m  -11010.00 ;8902,0,6058  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 65700: 8145.14 Train Score: 0.00 ;0,0,0 Loss: -0.129263 Valid Score: 2129.00 ;6862,1,8097 TEST Score: \u001b[47m \u001b[37m  -1771.00 ;6944,1,8015  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 66000: 8954.71 Train Score: 2433214.00 ;22165,1,17794 Loss: -0.129172 Valid Score: 30099.00 ;8152,1,6807 TEST Score: \u001b[47m \u001b[37m  17810.00 ;8308,0,6652  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 66300: 9182.40 Train Score: 0.00 ;0,0,0 Loss: -0.129216 Valid Score: 18759.00 ;7089,1,7870 TEST Score: \u001b[47m \u001b[37m  219.00 ;7207,1,7752  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 66600: 10462.22 Train Score: 0.00 ;0,0,0 Loss: -0.129465 Valid Score: 23628.00 ;7838,2,7120 TEST Score: \u001b[47m \u001b[37m  7479.00 ;7956,1,7003  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 66900: 10064.20 Train Score: 0.00 ;0,0,0 Loss: -0.129758 Valid Score: 24298.00 ;7419,2,7539 TEST Score: \u001b[47m \u001b[37m  10969.00 ;7564,1,7395  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 67200: 10522.78 Train Score: 0.00 ;0,0,0 Loss: -0.130457 Valid Score: 9768.00 ;7774,2,7184 TEST Score: \u001b[47m \u001b[37m  519.00 ;7939,1,7020  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 67500: 10675.46 Train Score: 2749938.00 ;20374,2,19584 Loss: -0.130466 Valid Score: 17338.00 ;7455,2,7503 TEST Score: \u001b[47m \u001b[37m  938.00 ;7636,2,7322  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 67800: 10147.94 Train Score: 0.00 ;0,0,0 Loss: -0.130836 Valid Score: 24538.00 ;7631,2,7327 TEST Score: \u001b[47m \u001b[37m  2799.00 ;7788,1,7171  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 68100: 11200.78 Train Score: 0.00 ;0,0,0 Loss: -0.131287 Valid Score: 3568.00 ;7544,2,7414 TEST Score: \u001b[47m \u001b[37m  3448.00 ;7684,2,7274  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 68400: 10586.16 Train Score: 0.00 ;0,0,0 Loss: -0.131447 Valid Score: 10158.00 ;7659,2,7299 TEST Score: \u001b[47m \u001b[37m  -5342.00 ;7776,2,7182  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 68700: 10202.42 Train Score: 0.00 ;0,0,0 Loss: -0.131992 Valid Score: 10758.00 ;7648,2,7310 TEST Score: \u001b[47m \u001b[37m  5477.00 ;7790,3,7167  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 69000: 10411.58 Train Score: 2840590.00 ;20780,5,19175 Loss: -0.132110 Valid Score: 7477.00 ;7637,3,7320 TEST Score: \u001b[47m \u001b[37m  5828.00 ;7788,2,7170  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 69300: 11263.41 Train Score: 0.00 ;0,0,0 Loss: -0.132190 Valid Score: 12275.00 ;7555,5,7400 TEST Score: \u001b[47m \u001b[37m  8036.00 ;7704,4,7252  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 69600: 10073.44 Train Score: 0.00 ;0,0,0 Loss: -0.132299 Valid Score: -3972.00 ;7524,2,7434 TEST Score: \u001b[47m \u001b[37m  -3282.00 ;7623,2,7335  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 69900: 11024.76 Train Score: 0.00 ;0,0,0 Loss: -0.132600 Valid Score: -2651.00 ;7597,1,7362 TEST Score: \u001b[47m \u001b[37m  9580.00 ;7726,0,7234  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 70200: 11269.85 Train Score: 0.00 ;0,0,0 Loss: -0.133074 Valid Score: 4870.00 ;7851,0,7109 TEST Score: \u001b[47m \u001b[37m  15710.00 ;7968,0,6992  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 70500: 9940.55 Train Score: 2915959.00 ;20898,1,19061 Loss: -0.133213 Valid Score: 15040.00 ;7681,0,7279 TEST Score: \u001b[47m \u001b[37m  9540.00 ;7815,0,7145  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 70800: 10632.01 Train Score: 0.00 ;0,0,0 Loss: -0.133241 Valid Score: 330.00 ;7387,0,7573 TEST Score: \u001b[47m \u001b[37m  16410.00 ;7530,0,7430  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 71100: 10035.16 Train Score: 0.00 ;0,0,0 Loss: -0.133420 Valid Score: 7690.00 ;7248,0,7712 TEST Score: \u001b[47m \u001b[37m  10110.00 ;7400,0,7560  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 71400: 8616.13 Train Score: 0.00 ;0,0,0 Loss: -0.133657 Valid Score: -13080.00 ;6395,0,8565 TEST Score: \u001b[47m \u001b[37m  -8321.00 ;6464,1,8495  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 71700: 8737.08 Train Score: 0.00 ;0,0,0 Loss: -0.133755 Valid Score: -5111.00 ;9259,1,5700 TEST Score: \u001b[47m \u001b[37m  13670.00 ;9395,0,5565  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 72000: 7596.34 Train Score: 2666669.00 ;22037,1,17922 Loss: -0.133580 Valid Score: 9319.00 ;8116,1,6843 TEST Score: \u001b[47m \u001b[37m  -21611.00 ;8249,1,6710  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 72300: 9503.03 Train Score: 0.00 ;0,0,0 Loss: -0.133894 Valid Score: 14060.00 ;6613,0,8347 TEST Score: \u001b[47m \u001b[37m  2339.00 ;6670,1,8289  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 72600: 9757.99 Train Score: 0.00 ;0,0,0 Loss: -0.134089 Valid Score: 22119.00 ;7805,1,7154 TEST Score: \u001b[47m \u001b[37m  8979.00 ;7918,1,7041  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 72900: 9528.74 Train Score: 0.00 ;0,0,0 Loss: -0.133985 Valid Score: 22099.00 ;7935,1,7024 TEST Score: \u001b[47m \u001b[37m  11899.00 ;8048,1,6911  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 73200: 10438.67 Train Score: 0.00 ;0,0,0 Loss: -0.134154 Valid Score: 7349.00 ;6302,1,8657 TEST Score: \u001b[47m \u001b[37m  -6052.00 ;6358,2,8600  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 73500: 8888.97 Train Score: 1909796.00 ;25094,9,14857 Loss: -0.134148 Valid Score: 5308.00 ;9287,2,5671 TEST Score: \u001b[47m \u001b[37m  -1032.00 ;9436,2,5522  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 73800: 8288.68 Train Score: 0.00 ;0,0,0 Loss: -0.134124 Valid Score: 6337.00 ;6044,3,8913 TEST Score: \u001b[47m \u001b[37m  8018.00 ;6078,2,8880  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 74100: 8016.09 Train Score: 0.00 ;0,0,0 Loss: -0.134101 Valid Score: 14228.00 ;8243,2,6715 TEST Score: \u001b[47m \u001b[37m  -6973.00 ;8387,3,6570  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 74400: 10530.50 Train Score: 0.00 ;0,0,0 Loss: -0.134581 Valid Score: 25187.00 ;7810,3,7147 TEST Score: \u001b[47m \u001b[37m  9668.00 ;7980,2,6978  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 74700: 9677.06 Train Score: 0.00 ;0,0,0 Loss: -0.134797 Valid Score: 7648.00 ;7246,2,7712 TEST Score: \u001b[47m \u001b[37m  7678.00 ;7406,2,7552  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 75000: 9479.67 Train Score: 2790027.00 ;20070,8,19882 Loss: -0.134944 Valid Score: 65.00 ;7376,5,7579 TEST Score: \u001b[47m \u001b[37m  3587.00 ;7505,3,7452  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 75300: 9899.85 Train Score: 0.00 ;0,0,0 Loss: -0.135189 Valid Score: 21145.00 ;7754,5,7201 TEST Score: \u001b[47m \u001b[37m  25987.00 ;7893,3,7064  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 75600: 9572.15 Train Score: 0.00 ;0,0,0 Loss: -0.135837 Valid Score: 12897.00 ;7958,3,6999 TEST Score: \u001b[47m \u001b[37m  20608.00 ;8082,2,6876  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 75900: 11502.93 Train Score: 0.00 ;0,0,0 Loss: -0.136386 Valid Score: 16298.00 ;7893,2,7065 TEST Score: \u001b[47m \u001b[37m  5408.00 ;8006,2,6952  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 76200: 12430.62 Train Score: 0.00 ;0,0,0 Loss: -0.136820 Valid Score: 3638.00 ;7536,2,7422 TEST Score: \u001b[47m \u001b[37m  -10682.00 ;7620,2,7338  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 76500: 10713.66 Train Score: 2896703.00 ;21045,2,18913 Loss: -0.137329 Valid Score: -1862.00 ;7775,2,7183 TEST Score: \u001b[47m \u001b[37m  3858.00 ;7893,2,7065  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 76800: 9382.53 Train Score: 0.00 ;0,0,0 Loss: -0.137456 Valid Score: 1768.00 ;7513,2,7445 TEST Score: \u001b[47m \u001b[37m  18658.00 ;7652,2,7306  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 77100: 10673.09 Train Score: 0.00 ;0,0,0 Loss: -0.137973 Valid Score: -8842.00 ;6669,2,8289 TEST Score: \u001b[47m \u001b[37m  -792.00 ;6802,2,8156  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 77400: 8989.47 Train Score: 0.00 ;0,0,0 Loss: -0.137997 Valid Score: 7298.00 ;7847,2,7111 TEST Score: \u001b[47m \u001b[37m  -3813.00 ;8001,3,6956  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 77700: 10323.46 Train Score: 0.00 ;0,0,0 Loss: -0.138448 Valid Score: 3998.00 ;8162,2,6796 TEST Score: \u001b[47m \u001b[37m  -3672.00 ;8286,2,6672  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 78000: 10172.79 Train Score: 2854648.00 ;21045,2,18913 Loss: -0.138845 Valid Score: 14200.00 ;7736,0,7224 TEST Score: \u001b[47m \u001b[37m  4419.00 ;7903,1,7056  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 78300: 10552.88 Train Score: 0.00 ;0,0,0 Loss: -0.139389 Valid Score: 5188.00 ;8999,2,5959 TEST Score: \u001b[47m \u001b[37m  16838.00 ;9163,2,5795  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 78600: 8931.59 Train Score: 0.00 ;0,0,0 Loss: -0.139248 Valid Score: -1812.00 ;5092,2,9866 TEST Score: \u001b[47m \u001b[37m  25159.00 ;5117,1,9842  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 78900: 7335.25 Train Score: 0.00 ;0,0,0 Loss: -0.139042 Valid Score: -11010.00 ;9689,0,5271 TEST Score: \u001b[47m \u001b[37m  13568.00 ;9923,2,5035  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 79200: 8304.47 Train Score: 0.00 ;0,0,0 Loss: -0.138950 Valid Score: -3061.00 ;5706,1,9253 TEST Score: \u001b[47m \u001b[37m  16708.00 ;5723,2,9235  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 79500: 7180.88 Train Score: 1521329.00 ;26776,6,13178 Loss: -0.138597 Valid Score: -902.00 ;9871,2,5087 TEST Score: \u001b[47m \u001b[37m  4338.00 ;10052,2,4906  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 79800: 7067.24 Train Score: 0.00 ;0,0,0 Loss: -0.138179 Valid Score: 820.00 ;6220,0,8740 TEST Score: \u001b[47m \u001b[37m  7240.00 ;6351,0,8609  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 80100: 8557.57 Train Score: 0.00 ;0,0,0 Loss: -0.137859 Valid Score: 11900.00 ;8620,0,6340 TEST Score: \u001b[47m \u001b[37m  -881.00 ;8780,1,6179  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 80400: 8442.42 Train Score: 0.00 ;0,0,0 Loss: -0.137660 Valid Score: 22280.00 ;7160,0,7800 TEST Score: \u001b[47m \u001b[37m  9320.00 ;7281,0,7679  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 80700: 9551.38 Train Score: 0.00 ;0,0,0 Loss: -0.137858 Valid Score: 7260.00 ;7220,0,7740 TEST Score: \u001b[47m \u001b[37m  19830.00 ;7338,0,7622  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 81000: 9817.98 Train Score: 2745024.00 ;21608,1,18351 Loss: -0.137699 Valid Score: -5060.00 ;7975,0,6985 TEST Score: \u001b[47m \u001b[37m  17470.00 ;8141,0,6819  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 81300: 10272.96 Train Score: 0.00 ;0,0,0 Loss: -0.137755 Valid Score: -2200.00 ;7623,0,7337 TEST Score: \u001b[47m \u001b[37m  10700.00 ;7794,0,7166  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 81600: 10205.62 Train Score: 0.00 ;0,0,0 Loss: -0.138020 Valid Score: -9580.00 ;7359,0,7601 TEST Score: \u001b[47m \u001b[37m  12500.00 ;7478,0,7482  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 81900: 10150.50 Train Score: 0.00 ;0,0,0 Loss: -0.138134 Valid Score: -1250.00 ;6363,0,8597 TEST Score: \u001b[47m \u001b[37m  -21040.00 ;6451,0,8509  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 82200: 8944.10 Train Score: 0.00 ;0,0,0 Loss: -0.137957 Valid Score: 8240.00 ;9010,0,5950 TEST Score: \u001b[47m \u001b[37m  -12460.00 ;9110,0,5850  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 82500: 9228.40 Train Score: 2653640.00 ;18711,0,21249 Loss: -0.137690 Valid Score: 12760.00 ;6895,0,8065 TEST Score: \u001b[47m \u001b[37m  -5320.00 ;6982,0,7978  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 82800: 9078.08 Train Score: 0.00 ;0,0,0 Loss: -0.137490 Valid Score: 14800.00 ;7254,0,7706 TEST Score: \u001b[47m \u001b[37m  4440.00 ;7381,0,7579  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 83100: 10945.03 Train Score: 0.00 ;0,0,0 Loss: -0.138055 Valid Score: 1520.00 ;8486,0,6474 TEST Score: \u001b[47m \u001b[37m  4860.00 ;8635,0,6325  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 83400: 8111.43 Train Score: 0.00 ;0,0,0 Loss: -0.138417 Valid Score: 5040.00 ;8351,0,6609 TEST Score: \u001b[47m \u001b[37m  21530.00 ;8499,0,6461  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 83700: 9781.55 Train Score: 0.00 ;0,0,0 Loss: -0.138801 Valid Score: 4630.00 ;6634,0,8326 TEST Score: \u001b[47m \u001b[37m  8140.00 ;6722,0,8238  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 84000: 10297.55 Train Score: 2780980.00 ;21773,0,18187 Loss: -0.139429 Valid Score: 8250.00 ;8035,0,6925 TEST Score: \u001b[47m \u001b[37m  9610.00 ;8149,0,6811  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 84300: 10084.23 Train Score: 0.00 ;0,0,0 Loss: -0.140029 Valid Score: 7890.00 ;7700,0,7260 TEST Score: \u001b[47m \u001b[37m  12500.00 ;7834,0,7126  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 84600: 10188.70 Train Score: 0.00 ;0,0,0 Loss: -0.140293 Valid Score: -7870.00 ;6601,0,8359 TEST Score: \u001b[47m \u001b[37m  -17160.00 ;6704,0,8256  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 84900: 10506.38 Train Score: 0.00 ;0,0,0 Loss: -0.140829 Valid Score: 3500.00 ;8113,0,6847 TEST Score: \u001b[47m \u001b[37m  11560.00 ;8259,0,6701  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 85200: 11076.67 Train Score: 0.00 ;0,0,0 Loss: -0.141104 Valid Score: 19790.00 ;8528,0,6432 TEST Score: \u001b[47m \u001b[37m  390.00 ;8677,0,6283  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 85500: 9430.30 Train Score: 2820700.00 ;22187,0,17773 Loss: -0.141631 Valid Score: 17310.00 ;8213,0,6747 TEST Score: \u001b[47m \u001b[37m  5870.00 ;8382,0,6578  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 85800: 9905.85 Train Score: 0.00 ;0,0,0 Loss: -0.141715 Valid Score: -2000.00 ;6417,0,8543 TEST Score: \u001b[47m \u001b[37m  -12330.00 ;6513,0,8447  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 86100: 9579.58 Train Score: 0.00 ;0,0,0 Loss: -0.141636 Valid Score: 6460.00 ;8214,0,6746 TEST Score: \u001b[47m \u001b[37m  15790.00 ;8340,0,6620  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 86400: 10035.28 Train Score: 0.00 ;0,0,0 Loss: -0.141871 Valid Score: -10390.00 ;7802,0,7158 TEST Score: \u001b[47m \u001b[37m  10340.00 ;7936,0,7024  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 86700: 11243.45 Train Score: 0.00 ;0,0,0 Loss: -0.142143 Valid Score: -5760.00 ;7658,0,7302 TEST Score: \u001b[47m \u001b[37m  17220.00 ;7775,0,7185  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 87000: 12185.48 Train Score: 2928170.00 ;19613,0,20347 Loss: -0.142426 Valid Score: 4760.00 ;7193,0,7767 TEST Score: \u001b[47m \u001b[37m  23530.00 ;7308,0,7652  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 87300: 10879.60 Train Score: 0.00 ;0,0,0 Loss: -0.143059 Valid Score: 3160.00 ;6971,0,7989 TEST Score: \u001b[47m \u001b[37m  -6550.00 ;7082,0,7878  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 87600: 10504.23 Train Score: 0.00 ;0,0,0 Loss: -0.142858 Valid Score: 1640.00 ;7536,0,7424 TEST Score: \u001b[47m \u001b[37m  20230.00 ;7641,0,7319  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 87900: 11936.13 Train Score: 0.00 ;0,0,0 Loss: -0.142959 Valid Score: 800.00 ;7461,0,7499 TEST Score: \u001b[47m \u001b[37m  18060.00 ;7562,0,7398  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 88200: 11636.88 Train Score: 0.00 ;0,0,0 Loss: -0.143257 Valid Score: 5330.00 ;7455,0,7505 TEST Score: \u001b[47m \u001b[37m  1620.00 ;7572,0,7388  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 88500: 11526.78 Train Score: 2979675.00 ;21355,0,18605 Loss: -0.143584 Valid Score: -16470.00 ;7909,0,7051 TEST Score: \u001b[47m \u001b[37m  17170.00 ;7993,0,6967  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 88800: 11210.83 Train Score: 0.00 ;0,0,0 Loss: -0.143995 Valid Score: 6010.00 ;7511,0,7449 TEST Score: \u001b[47m \u001b[37m  20230.00 ;7645,0,7315  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 89100: 11006.57 Train Score: 0.00 ;0,0,0 Loss: -0.144166 Valid Score: 5260.00 ;7498,0,7462 TEST Score: \u001b[47m \u001b[37m  18720.00 ;7619,0,7341  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 89400: 11658.10 Train Score: 0.00 ;0,0,0 Loss: -0.144123 Valid Score: 510.00 ;7543,0,7417 TEST Score: \u001b[47m \u001b[37m  6070.00 ;7624,0,7336  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 89700: 9845.42 Train Score: 0.00 ;0,0,0 Loss: -0.144334 Valid Score: -13720.00 ;7888,0,7072 TEST Score: \u001b[47m \u001b[37m  9450.00 ;8010,0,6950  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 90000: 10045.32 Train Score: 2466135.00 ;23402,0,16558 Loss: -0.144651 Valid Score: 18980.00 ;8723,0,6237 TEST Score: \u001b[47m \u001b[37m  6570.00 ;8850,0,6110  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 90300: 9848.10 Train Score: 0.00 ;0,0,0 Loss: -0.144799 Valid Score: -3380.00 ;8602,0,6358 TEST Score: \u001b[47m \u001b[37m  -6260.00 ;8751,0,6209  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 90600: 8815.82 Train Score: 0.00 ;0,0,0 Loss: -0.144871 Valid Score: 8860.00 ;7501,0,7459 TEST Score: \u001b[47m \u001b[37m  4290.00 ;7593,0,7367  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 90900: 10870.17 Train Score: 0.00 ;0,0,0 Loss: -0.144860 Valid Score: 5830.00 ;6631,0,8329 TEST Score: \u001b[47m \u001b[37m  -11290.00 ;6713,0,8247  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 91200: 9907.30 Train Score: 0.00 ;0,0,0 Loss: -0.145038 Valid Score: -4370.00 ;6213,0,8747 TEST Score: \u001b[47m \u001b[37m  -17970.00 ;6274,0,8686  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 91500: 8719.37 Train Score: 2616770.00 ;22927,0,17033 Loss: -0.144637 Valid Score: 820.00 ;8543,0,6417 TEST Score: \u001b[47m \u001b[37m  -2740.00 ;8667,0,6293  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 91800: 10236.48 Train Score: 0.00 ;0,0,0 Loss: -0.144671 Valid Score: 4120.00 ;8508,0,6452 TEST Score: \u001b[47m \u001b[37m  -1760.00 ;8644,0,6316  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 92100: 10197.47 Train Score: 0.00 ;0,0,0 Loss: -0.145207 Valid Score: 15840.00 ;6650,0,8310 TEST Score: \u001b[47m \u001b[37m  -15990.00 ;6657,0,8303  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 92400: 10597.63 Train Score: 0.00 ;0,0,0 Loss: -0.146372 Valid Score: 10240.00 ;6770,0,8190 TEST Score: \u001b[47m \u001b[37m  -8610.00 ;6852,0,8108  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 92700: 9801.05 Train Score: 0.00 ;0,0,0 Loss: -0.147387 Valid Score: 550.00 ;8625,0,6335 TEST Score: \u001b[47m \u001b[37m  -12400.00 ;8751,0,6209  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 93000: 10554.02 Train Score: 2679065.00 ;18437,0,21523 Loss: -0.148005 Valid Score: 14490.00 ;6755,0,8205 TEST Score: \u001b[47m \u001b[37m  1830.00 ;6860,0,8100  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 93300: 10005.87 Train Score: 0.00 ;0,0,0 Loss: -0.148847 Valid Score: 16070.00 ;7265,0,7695 TEST Score: \u001b[47m \u001b[37m  -4770.00 ;7361,0,7599  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 93600: 10888.43 Train Score: 0.00 ;0,0,0 Loss: -0.149296 Valid Score: 3050.00 ;8163,0,6797 TEST Score: \u001b[47m \u001b[37m  -3180.00 ;8293,0,6667  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 93900: 10396.88 Train Score: 0.00 ;0,0,0 Loss: -0.149986 Valid Score: 4050.00 ;7632,0,7328 TEST Score: \u001b[47m \u001b[37m  16080.00 ;7758,0,7202  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 94200: 10963.05 Train Score: 0.00 ;0,0,0 Loss: -0.150724 Valid Score: 320.00 ;7834,0,7126 TEST Score: \u001b[47m \u001b[37m  16390.00 ;8009,0,6951  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 94500: 11107.30 Train Score: 2939535.00 ;19646,0,20314 Loss: -0.151286 Valid Score: 8410.00 ;7227,0,7733 TEST Score: \u001b[47m \u001b[37m  17710.00 ;7344,0,7616  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 94800: 11747.50 Train Score: 0.00 ;0,0,0 Loss: -0.151965 Valid Score: -190.00 ;7339,0,7621 TEST Score: \u001b[47m \u001b[37m  11600.00 ;7415,0,7545  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 95100: 10644.00 Train Score: 0.00 ;0,0,0 Loss: -0.152448 Valid Score: 2780.00 ;7918,0,7042 TEST Score: \u001b[47m \u001b[37m  1060.00 ;8031,0,6929  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 95400: 11009.77 Train Score: 0.00 ;0,0,0 Loss: -0.152985 Valid Score: -3050.00 ;7587,0,7373 TEST Score: \u001b[47m \u001b[37m  17100.00 ;7680,0,7280  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 95700: 11397.17 Train Score: 0.00 ;0,0,0 Loss: -0.153826 Valid Score: -2810.00 ;7286,0,7674 TEST Score: \u001b[47m \u001b[37m  11510.00 ;7396,0,7564  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 96000: 9513.13 Train Score: 2961705.00 ;19543,0,20417 Loss: -0.154348 Valid Score: 8770.00 ;7165,0,7795 TEST Score: \u001b[47m \u001b[37m  8800.00 ;7249,0,7711  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 96300: 12595.27 Train Score: 0.00 ;0,0,0 Loss: -0.155123 Valid Score: 4580.00 ;6336,0,8624 TEST Score: \u001b[47m \u001b[37m  -9050.00 ;6433,0,8527  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 96600: 9133.15 Train Score: 0.00 ;0,0,0 Loss: -0.155018 Valid Score: -28790.00 ;8492,0,6468 TEST Score: \u001b[47m \u001b[37m  1020.00 ;8631,0,6329  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 96900: 10422.12 Train Score: 0.00 ;0,0,0 Loss: -0.154999 Valid Score: -28810.00 ;8398,0,6562 TEST Score: \u001b[47m \u001b[37m  15380.00 ;8538,0,6422  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 97200: 9661.27 Train Score: 0.00 ;0,0,0 Loss: -0.154783 Valid Score: 13380.00 ;6890,0,8070 TEST Score: \u001b[47m \u001b[37m  -14530.00 ;6976,0,7984  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 97500: 9188.03 Train Score: 2843655.00 ;19176,0,20784 Loss: -0.154588 Valid Score: 2900.00 ;7033,0,7927 TEST Score: \u001b[47m \u001b[37m  -17140.00 ;7122,0,7838  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 97800: 10228.48 Train Score: 0.00 ;0,0,0 Loss: -0.154582 Valid Score: -14390.00 ;8704,0,6256 TEST Score: \u001b[47m \u001b[37m  -10550.00 ;8814,0,6146  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 98100: 9188.38 Train Score: 0.00 ;0,0,0 Loss: -0.154156 Valid Score: 2890.00 ;7755,0,7205 TEST Score: \u001b[47m \u001b[37m  -70.00 ;7914,0,7046  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 98400: 11271.92 Train Score: 0.00 ;0,0,0 Loss: -0.154324 Valid Score: 8760.00 ;6941,0,8019 TEST Score: \u001b[47m \u001b[37m  -5210.00 ;7039,0,7921  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 98700: 10582.47 Train Score: 0.00 ;0,0,0 Loss: -0.154181 Valid Score: 14480.00 ;7242,0,7718 TEST Score: \u001b[47m \u001b[37m  10200.00 ;7340,0,7620  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 99000: 10564.85 Train Score: 2924655.00 ;21654,0,18306 Loss: -0.154172 Valid Score: 5290.00 ;8024,0,6936 TEST Score: \u001b[47m \u001b[37m  3110.00 ;8147,0,6813  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 99300: 10835.18 Train Score: 0.00 ;0,0,0 Loss: -0.154073 Valid Score: 6250.00 ;7652,0,7308 TEST Score: \u001b[47m \u001b[37m  15850.00 ;7767,0,7193  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 99600: 11610.12 Train Score: 0.00 ;0,0,0 Loss: -0.154476 Valid Score: 7390.00 ;7718,0,7242 TEST Score: \u001b[47m \u001b[37m  14900.00 ;7825,0,7135  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 99900: 11196.45 Train Score: 0.00 ;0,0,0 Loss: -0.154615 Valid Score: -10200.00 ;7964,0,6996 TEST Score: \u001b[47m \u001b[37m  10940.00 ;8104,0,6856  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 100200: 11944.78 Train Score: 0.00 ;0,0,0 Loss: -0.154681 Valid Score: 21090.00 ;7440,0,7520 TEST Score: \u001b[47m \u001b[37m  26200.00 ;7582,0,7378  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 100500: 10505.37 Train Score: 3058780.00 ;19823,0,20137 Loss: -0.154693 Valid Score: 11560.00 ;7303,0,7657 TEST Score: \u001b[47m \u001b[37m  20340.00 ;7409,0,7551  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 100800: 10742.30 Train Score: 0.00 ;0,0,0 Loss: -0.154906 Valid Score: 5140.00 ;6953,0,8007 TEST Score: \u001b[47m \u001b[37m  -3810.00 ;7052,0,7908  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 101100: 10324.10 Train Score: 0.00 ;0,0,0 Loss: -0.155032 Valid Score: 10860.00 ;6550,0,8410 TEST Score: \u001b[47m \u001b[37m  -9200.00 ;6644,0,8316  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 101400: 8116.90 Train Score: 0.00 ;0,0,0 Loss: -0.154939 Valid Score: 2590.00 ;8001,0,6959 TEST Score: \u001b[47m \u001b[37m  6050.00 ;8125,0,6835  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 101700: 9264.80 Train Score: 0.00 ;0,0,0 Loss: -0.155238 Valid Score: -17540.00 ;9729,0,5231 TEST Score: \u001b[47m \u001b[37m  -1970.00 ;9940,0,5020  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 102000: 6805.63 Train Score: 2182915.00 ;16883,0,23077 Loss: -0.155130 Valid Score: -7520.00 ;6224,0,8736 TEST Score: \u001b[47m \u001b[37m  9600.00 ;6294,0,8666  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 102300: 7766.23 Train Score: 0.00 ;0,0,0 Loss: -0.154825 Valid Score: 11670.00 ;8651,0,6309 TEST Score: \u001b[47m \u001b[37m  3700.00 ;8764,0,6196  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 102600: 9093.03 Train Score: 0.00 ;0,0,0 Loss: -0.154459 Valid Score: 12150.00 ;6578,0,8382 TEST Score: \u001b[47m \u001b[37m  25030.00 ;6668,0,8292  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 102900: 9631.10 Train Score: 0.00 ;0,0,0 Loss: -0.154482 Valid Score: 920.00 ;8185,0,6775 TEST Score: \u001b[47m \u001b[37m  11410.00 ;8329,0,6631  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 103200: 10874.60 Train Score: 0.00 ;0,0,0 Loss: -0.154452 Valid Score: 9200.00 ;7662,0,7298 TEST Score: \u001b[47m \u001b[37m  9220.00 ;7759,0,7201  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 103500: 11579.98 Train Score: 2988140.00 ;20149,0,19811 Loss: -0.154845 Valid Score: 6830.00 ;7392,0,7568 TEST Score: \u001b[47m \u001b[37m  12390.00 ;7511,0,7449  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 103800: 12138.22 Train Score: 0.00 ;0,0,0 Loss: -0.155320 Valid Score: 1470.00 ;7457,0,7503 TEST Score: \u001b[47m \u001b[37m  13550.00 ;7541,0,7419  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 104100: 11908.58 Train Score: 0.00 ;0,0,0 Loss: -0.155905 Valid Score: 3450.00 ;6798,0,8162 TEST Score: \u001b[47m \u001b[37m  -1910.00 ;6863,0,8097  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 104400: 10803.05 Train Score: 0.00 ;0,0,0 Loss: -0.155817 Valid Score: 4310.00 ;6813,0,8147 TEST Score: \u001b[47m \u001b[37m  -4550.00 ;6902,0,8058  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 104700: 9643.08 Train Score: 0.00 ;0,0,0 Loss: -0.155863 Valid Score: 5020.00 ;8079,0,6881 TEST Score: \u001b[47m \u001b[37m  23460.00 ;8214,0,6746  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 105000: 6748.17 Train Score: 2583330.00 ;17941,0,22019 Loss: -0.155708 Valid Score: 15210.00 ;6584,0,8376 TEST Score: \u001b[47m \u001b[37m  -9180.00 ;6691,0,8269  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 105300: 7955.22 Train Score: 0.00 ;0,0,0 Loss: -0.155464 Valid Score: -9410.00 ;10357,0,4603 TEST Score: \u001b[47m \u001b[37m  -33210.00 ;10571,0,4389  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 105600: 7058.05 Train Score: 0.00 ;0,0,0 Loss: -0.155049 Valid Score: 32020.00 ;5646,0,9314 TEST Score: \u001b[47m \u001b[37m  -10300.00 ;5660,0,9300  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 105900: 5719.23 Train Score: 0.00 ;0,0,0 Loss: -0.153969 Valid Score: 37780.00 ;6132,0,8828 TEST Score: \u001b[47m \u001b[37m  -4610.00 ;6171,0,8789  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 106200: 7680.52 Train Score: 0.00 ;0,0,0 Loss: -0.153449 Valid Score: 6770.00 ;8733,0,6227 TEST Score: \u001b[47m \u001b[37m  3380.00 ;8907,0,6053  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 106500: 7500.87 Train Score: 2461325.00 ;18691,0,21269 Loss: -0.152707 Valid Score: 25810.00 ;6815,0,8145 TEST Score: \u001b[47m \u001b[37m  6610.00 ;6949,0,8011  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 106800: 8843.67 Train Score: 0.00 ;0,0,0 Loss: -0.152468 Valid Score: 3420.00 ;8327,0,6633 TEST Score: \u001b[47m \u001b[37m  15680.00 ;8433,0,6527  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 107100: 9752.35 Train Score: 0.00 ;0,0,0 Loss: -0.152182 Valid Score: 12770.00 ;6990,0,7970 TEST Score: \u001b[47m \u001b[37m  21090.00 ;7101,0,7859  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 107400: 9845.17 Train Score: 0.00 ;0,0,0 Loss: -0.152285 Valid Score: -20670.00 ;8316,0,6644 TEST Score: \u001b[47m \u001b[37m  6130.00 ;8472,0,6488  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 107700: 9106.58 Train Score: 0.00 ;0,0,0 Loss: -0.152122 Valid Score: -7870.00 ;7243,0,7717 TEST Score: \u001b[47m \u001b[37m  1810.00 ;7317,0,7643  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 108000: 10470.32 Train Score: 2998600.00 ;19595,0,20365 Loss: -0.152069 Valid Score: -9800.00 ;7195,0,7765 TEST Score: \u001b[47m \u001b[37m  4710.00 ;7284,0,7676  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 108300: 11269.42 Train Score: 0.00 ;0,0,0 Loss: -0.152131 Valid Score: -4630.00 ;7352,0,7608 TEST Score: \u001b[47m \u001b[37m  12540.00 ;7446,0,7514  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 108600: 11115.38 Train Score: 0.00 ;0,0,0 Loss: -0.152643 Valid Score: -13280.00 ;7524,0,7436 TEST Score: \u001b[47m \u001b[37m  10730.00 ;7650,0,7310  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 108900: 11016.35 Train Score: 0.00 ;0,0,0 Loss: -0.153429 Valid Score: -2570.00 ;7570,0,7390 TEST Score: \u001b[47m \u001b[37m  6650.00 ;7731,0,7229  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 109200: 11702.58 Train Score: 0.00 ;0,0,0 Loss: -0.154230 Valid Score: -8770.00 ;7788,0,7172 TEST Score: \u001b[47m \u001b[37m  1950.00 ;7929,0,7031  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 109500: 11302.77 Train Score: 3148495.00 ;20511,0,19449 Loss: -0.154632 Valid Score: -4610.00 ;7533,0,7427 TEST Score: \u001b[47m \u001b[37m  8420.00 ;7661,0,7299  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 109800: 11451.92 Train Score: 0.00 ;0,0,0 Loss: -0.155377 Valid Score: 5330.00 ;7845,0,7115 TEST Score: \u001b[47m \u001b[37m  14460.00 ;7952,0,7008  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 110100: 12336.72 Train Score: 0.00 ;0,0,0 Loss: -0.155959 Valid Score: -1070.00 ;7976,0,6984 TEST Score: \u001b[47m \u001b[37m  14650.00 ;8131,0,6829  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 110400: 12231.20 Train Score: 0.00 ;0,0,0 Loss: -0.156246 Valid Score: -1090.00 ;8163,0,6797 TEST Score: \u001b[47m \u001b[37m  3530.00 ;8306,0,6654  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 110700: 10100.77 Train Score: 0.00 ;0,0,0 Loss: -0.156598 Valid Score: -6390.00 ;8277,0,6683 TEST Score: \u001b[47m \u001b[37m  570.00 ;8416,0,6544  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 111000: 8922.40 Train Score: 3035145.00 ;20494,0,19466 Loss: -0.156540 Valid Score: -1000.00 ;7566,0,7394 TEST Score: \u001b[47m \u001b[37m  5610.00 ;7695,0,7265  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 111300: 9200.90 Train Score: 0.00 ;0,0,0 Loss: -0.156380 Valid Score: 12690.00 ;6514,0,8446 TEST Score: \u001b[47m \u001b[37m  -1420.00 ;6554,0,8406  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 111600: 8254.97 Train Score: 0.00 ;0,0,0 Loss: -0.155846 Valid Score: -15920.00 ;8289,0,6671 TEST Score: \u001b[47m \u001b[37m  -4780.00 ;8470,0,6490  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 111900: 10318.93 Train Score: 0.00 ;0,0,0 Loss: -0.155902 Valid Score: 39500.00 ;6589,0,8371 TEST Score: \u001b[47m \u001b[37m  6550.00 ;6643,0,8317  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 112200: 10567.37 Train Score: 0.00 ;0,0,0 Loss: -0.156103 Valid Score: 8400.00 ;7789,0,7171 TEST Score: \u001b[47m \u001b[37m  10470.00 ;7960,0,7000  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 112500: 11402.40 Train Score: 2653705.00 ;22678,0,17282 Loss: -0.156593 Valid Score: 11320.00 ;8407,0,6553 TEST Score: \u001b[47m \u001b[37m  1410.00 ;8558,0,6402  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 112800: 9648.88 Train Score: 0.00 ;0,0,0 Loss: -0.156676 Valid Score: 8090.00 ;6680,0,8280 TEST Score: \u001b[47m \u001b[37m  6140.00 ;6773,0,8187  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 113100: 10551.95 Train Score: 0.00 ;0,0,0 Loss: -0.156688 Valid Score: 1240.00 ;7178,0,7782 TEST Score: \u001b[47m \u001b[37m  3830.00 ;7278,0,7682  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 113400: 10802.68 Train Score: 0.00 ;0,0,0 Loss: -0.156997 Valid Score: 3970.00 ;8079,0,6881 TEST Score: \u001b[47m \u001b[37m  7360.00 ;8205,0,6755  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 113700: 9796.33 Train Score: 0.00 ;0,0,0 Loss: -0.157418 Valid Score: 14040.00 ;8355,0,6605 TEST Score: \u001b[47m \u001b[37m  10570.00 ;8456,0,6504  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 114000: 10470.80 Train Score: 3084870.00 ;20373,0,19587 Loss: -0.157437 Valid Score: 10980.00 ;7505,0,7455 TEST Score: \u001b[47m \u001b[37m  6330.00 ;7586,0,7374  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 114300: 11162.23 Train Score: 0.00 ;0,0,0 Loss: -0.157569 Valid Score: 7200.00 ;7657,0,7303 TEST Score: \u001b[47m \u001b[37m  19770.00 ;7767,0,7193  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 114600: 12890.60 Train Score: 0.00 ;0,0,0 Loss: -0.158269 Valid Score: 20120.00 ;7473,0,7487 TEST Score: \u001b[47m \u001b[37m  13680.00 ;7606,0,7354  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 114900: 11158.57 Train Score: 0.00 ;0,0,0 Loss: -0.158495 Valid Score: 6900.00 ;7641,0,7319 TEST Score: \u001b[47m \u001b[37m  19520.00 ;7776,0,7184  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 115200: 10586.05 Train Score: 0.00 ;0,0,0 Loss: -0.158578 Valid Score: 4620.00 ;7450,0,7510 TEST Score: \u001b[47m \u001b[37m  7290.00 ;7535,0,7425  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 115500: 11728.23 Train Score: 2478475.00 ;17532,0,22428 Loss: -0.159068 Valid Score: 10910.00 ;6406,0,8554 TEST Score: \u001b[47m \u001b[37m  -5860.00 ;6519,0,8441  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 115800: 10994.08 Train Score: 0.00 ;0,0,0 Loss: -0.159428 Valid Score: 14940.00 ;7030,0,7930 TEST Score: \u001b[47m \u001b[37m  9660.00 ;7127,0,7833  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 116100: 10555.35 Train Score: 0.00 ;0,0,0 Loss: -0.159828 Valid Score: 17200.00 ;7802,0,7158 TEST Score: \u001b[47m \u001b[37m  16780.00 ;7905,0,7055  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 116400: 11247.92 Train Score: 0.00 ;0,0,0 Loss: -0.159886 Valid Score: 4880.00 ;7691,0,7269 TEST Score: \u001b[47m \u001b[37m  8030.00 ;7834,0,7126  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 116700: 12240.85 Train Score: 0.00 ;0,0,0 Loss: -0.160251 Valid Score: 4720.00 ;8299,0,6661 TEST Score: \u001b[47m \u001b[37m  -3220.00 ;8439,0,6521  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 117000: 12310.03 Train Score: 2892695.00 ;22176,0,17784 Loss: -0.160299 Valid Score: 9580.00 ;8207,0,6753 TEST Score: \u001b[47m \u001b[37m  -8240.00 ;8348,0,6612  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 117300: 12047.72 Train Score: 0.00 ;0,0,0 Loss: -0.160021 Valid Score: 11680.00 ;7686,0,7274 TEST Score: \u001b[47m \u001b[37m  6200.00 ;7835,0,7125  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 117600: 12059.02 Train Score: 0.00 ;0,0,0 Loss: -0.160344 Valid Score: 2490.00 ;7710,0,7250 TEST Score: \u001b[47m \u001b[37m  18440.00 ;7855,0,7105  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 117900: 12381.43 Train Score: 0.00 ;0,0,0 Loss: -0.160676 Valid Score: 7620.00 ;7274,0,7686 TEST Score: \u001b[47m \u001b[37m  17820.00 ;7431,0,7529  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 118200: 12755.55 Train Score: 0.00 ;0,0,0 Loss: -0.160745 Valid Score: 24900.00 ;7126,0,7834 TEST Score: \u001b[47m \u001b[37m  10200.00 ;7260,0,7700  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 118500: 12507.53 Train Score: 3033405.00 ;19302,0,20658 Loss: -0.160849 Valid Score: 19900.00 ;7096,0,7864 TEST Score: \u001b[47m \u001b[37m  1670.00 ;7224,0,7736  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 118800: 10487.08 Train Score: 0.00 ;0,0,0 Loss: -0.160526 Valid Score: 6550.00 ;7392,0,7568 TEST Score: \u001b[47m \u001b[37m  20340.00 ;7496,0,7464  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 119100: 11685.33 Train Score: 0.00 ;0,0,0 Loss: -0.161032 Valid Score: 10200.00 ;7214,0,7746 TEST Score: \u001b[47m \u001b[37m  18360.00 ;7323,0,7637  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 119400: 10736.20 Train Score: 0.00 ;0,0,0 Loss: -0.161212 Valid Score: 4100.00 ;9079,0,5881 TEST Score: \u001b[47m \u001b[37m  -16780.00 ;9228,0,5732  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 119700: 9328.37 Train Score: 0.00 ;0,0,0 Loss: -0.161062 Valid Score: 1100.00 ;8932,0,6028 TEST Score: \u001b[47m \u001b[37m  -10600.00 ;9084,0,5876  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 120000: 9005.33 Train Score: 2045395.00 ;15958,0,24002 Loss: -0.160932 Valid Score: 12940.00 ;5948,0,9012 TEST Score: \u001b[47m \u001b[37m  -23630.00 ;5961,0,8999  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 120300: 9328.23 Train Score: 0.00 ;0,0,0 Loss: -0.160606 Valid Score: -3210.00 ;8804,0,6156 TEST Score: \u001b[47m \u001b[37m  -9880.00 ;8946,0,6014  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 120600: 11091.67 Train Score: 0.00 ;0,0,0 Loss: -0.160663 Valid Score: 35990.00 ;7307,0,7653 TEST Score: \u001b[47m \u001b[37m  9500.00 ;7442,0,7518  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 120900: 10044.85 Train Score: 0.00 ;0,0,0 Loss: -0.160332 Valid Score: 26860.00 ;6748,0,8212 TEST Score: \u001b[47m \u001b[37m  -13630.00 ;6831,0,8129  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 121200: 10708.63 Train Score: 0.00 ;0,0,0 Loss: -0.160160 Valid Score: 17190.00 ;7876,0,7084 TEST Score: \u001b[47m \u001b[37m  26230.00 ;8017,0,6943  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 121500: 11405.63 Train Score: 2947115.00 ;21910,0,18050 Loss: -0.160599 Valid Score: -160.00 ;8119,0,6841 TEST Score: \u001b[47m \u001b[37m  13470.00 ;8276,0,6684  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 121800: 10453.32 Train Score: 0.00 ;0,0,0 Loss: -0.160811 Valid Score: 33440.00 ;6884,0,8076 TEST Score: \u001b[47m \u001b[37m  -7400.00 ;7017,0,7943  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 122100: 11000.47 Train Score: 0.00 ;0,0,0 Loss: -0.160946 Valid Score: 18950.00 ;6468,0,8492 TEST Score: \u001b[47m \u001b[37m  -22260.00 ;6529,0,8431  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 122400: 9647.77 Train Score: 0.00 ;0,0,0 Loss: -0.160878 Valid Score: 4820.00 ;7731,0,7229 TEST Score: \u001b[47m \u001b[37m  22180.00 ;7846,0,7114  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 122700: 10365.18 Train Score: 0.00 ;0,0,0 Loss: -0.161131 Valid Score: 9550.00 ;8087,0,6873 TEST Score: \u001b[47m \u001b[37m  12910.00 ;8261,0,6699  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 123000: 12178.28 Train Score: 2955755.00 ;22148,0,17812 Loss: -0.161595 Valid Score: 6760.00 ;8223,0,6737 TEST Score: \u001b[47m \u001b[37m  8610.00 ;8366,0,6594  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 123300: 11625.15 Train Score: 0.00 ;0,0,0 Loss: -0.161830 Valid Score: 20310.00 ;7231,0,7729 TEST Score: \u001b[47m \u001b[37m  10220.00 ;7353,0,7607  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 123600: 10295.10 Train Score: 0.00 ;0,0,0 Loss: -0.161963 Valid Score: 8220.00 ;6287,0,8673 TEST Score: \u001b[47m \u001b[37m  2480.00 ;6334,0,8626  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 123900: 10044.65 Train Score: 0.00 ;0,0,0 Loss: -0.162016 Valid Score: 2780.00 ;7915,0,7045 TEST Score: \u001b[47m \u001b[37m  10240.00 ;8009,0,6951  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 124200: 11267.12 Train Score: 0.00 ;0,0,0 Loss: -0.162087 Valid Score: 4910.00 ;8080,0,6880 TEST Score: \u001b[47m \u001b[37m  5360.00 ;8217,0,6743  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 124500: 10451.50 Train Score: 3189820.00 ;20871,0,19089 Loss: -0.162220 Valid Score: 9940.00 ;7669,0,7291 TEST Score: \u001b[47m \u001b[37m  21000.00 ;7824,0,7136  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 124800: 11849.02 Train Score: 0.00 ;0,0,0 Loss: -0.162416 Valid Score: 9270.00 ;7389,0,7571 TEST Score: \u001b[47m \u001b[37m  -2900.00 ;7514,0,7446  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 125100: 12546.58 Train Score: 0.00 ;0,0,0 Loss: -0.162677 Valid Score: 11240.00 ;7077,0,7883 TEST Score: \u001b[47m \u001b[37m  4490.00 ;7159,0,7801  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 125400: 11608.95 Train Score: 0.00 ;0,0,0 Loss: -0.162693 Valid Score: 11260.00 ;6820,0,8140 TEST Score: \u001b[47m \u001b[37m  6660.00 ;6877,0,8083  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 125700: 11871.65 Train Score: 0.00 ;0,0,0 Loss: -0.162349 Valid Score: 8410.00 ;7551,0,7409 TEST Score: \u001b[47m \u001b[37m  14970.00 ;7706,0,7254  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 126000: 10189.18 Train Score: 2554485.00 ;23344,0,16616 Loss: -0.162134 Valid Score: -11050.00 ;8692,0,6268 TEST Score: \u001b[47m \u001b[37m  12000.00 ;8825,0,6135  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 126300: 9794.77 Train Score: 0.00 ;0,0,0 Loss: -0.161411 Valid Score: 12580.00 ;6254,0,8706 TEST Score: \u001b[47m \u001b[37m  -14200.00 ;6327,0,8633  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 126600: 6887.33 Train Score: 0.00 ;0,0,0 Loss: -0.161265 Valid Score: 14840.00 ;9766,0,5194 TEST Score: \u001b[47m \u001b[37m  -4080.00 ;9943,0,5017  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 126900: 7600.63 Train Score: 0.00 ;0,0,0 Loss: -0.161161 Valid Score: -6120.00 ;4895,0,10065 TEST Score: \u001b[47m \u001b[37m  37790.00 ;4921,0,10039  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 127200: 6778.52 Train Score: 0.00 ;0,0,0 Loss: -0.160883 Valid Score: 750.00 ;9125,0,5835 TEST Score: \u001b[47m \u001b[37m  -16700.00 ;9234,0,5726  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 127500: 6447.71 Train Score: 2699295.00 ;21193,0,18767 Loss: -0.160511 Valid Score: 12390.00 ;7740,0,7220 TEST Score: \u001b[47m \u001b[37m  -2960.00 ;7899,0,7061  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 127800: 8518.02 Train Score: 0.00 ;0,0,0 Loss: -0.160468 Valid Score: 9960.00 ;6720,0,8240 TEST Score: \u001b[47m \u001b[37m  6060.00 ;6768,0,8192  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 128100: 10406.57 Train Score: 0.00 ;0,0,0 Loss: -0.160546 Valid Score: -2620.00 ;8576,0,6384 TEST Score: \u001b[47m \u001b[37m  2640.00 ;8726,0,6234  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 128400: 7819.15 Train Score: 0.00 ;0,0,0 Loss: -0.160360 Valid Score: 11190.00 ;6410,0,8550 TEST Score: \u001b[47m \u001b[37m  12610.00 ;6501,0,8459  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 128700: 10007.25 Train Score: 0.00 ;0,0,0 Loss: -0.160090 Valid Score: 19110.00 ;8481,0,6479 TEST Score: \u001b[47m \u001b[37m  -1410.00 ;8637,0,6323  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 129000: 8811.80 Train Score: 2728030.00 ;18741,0,21219 Loss: -0.160117 Valid Score: 18000.00 ;6836,0,8124 TEST Score: \u001b[47m \u001b[37m  10980.00 ;6979,0,7981  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 129300: 10569.53 Train Score: 0.00 ;0,0,0 Loss: -0.160711 Valid Score: 11020.00 ;7494,0,7466 TEST Score: \u001b[47m \u001b[37m  13250.00 ;7667,0,7293  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 129600: 9325.32 Train Score: 0.00 ;0,0,0 Loss: -0.160566 Valid Score: -4250.00 ;8054,0,6906 TEST Score: \u001b[47m \u001b[37m  10190.00 ;8211,0,6749  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 129900: 9841.60 Train Score: 0.00 ;0,0,0 Loss: -0.160325 Valid Score: 5040.00 ;7146,0,7814 TEST Score: \u001b[47m \u001b[37m  19310.00 ;7256,0,7704  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 130200: 11589.13 Train Score: 0.00 ;0,0,0 Loss: -0.160562 Valid Score: 9150.00 ;7683,0,7277 TEST Score: \u001b[47m \u001b[37m  29600.00 ;7828,0,7132  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 130500: 10659.20 Train Score: 3095980.00 ;19745,0,20215 Loss: -0.160639 Valid Score: -3750.00 ;7245,0,7715 TEST Score: \u001b[47m \u001b[37m  14790.00 ;7369,0,7591  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 130800: 11831.20 Train Score: 0.00 ;0,0,0 Loss: -0.160638 Valid Score: -160.00 ;6879,0,8081 TEST Score: \u001b[47m \u001b[37m  4190.00 ;6981,0,7979  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 131100: 11281.28 Train Score: 0.00 ;0,0,0 Loss: -0.160732 Valid Score: 8130.00 ;7378,0,7582 TEST Score: \u001b[47m \u001b[37m  7020.00 ;7512,0,7448  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 131400: 13339.25 Train Score: 0.00 ;0,0,0 Loss: -0.161288 Valid Score: -8680.00 ;7882,0,7078 TEST Score: \u001b[47m \u001b[37m  12960.00 ;7990,0,6970  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 131700: 11804.73 Train Score: 0.00 ;0,0,0 Loss: -0.161463 Valid Score: 7030.00 ;7834,0,7126 TEST Score: \u001b[47m \u001b[37m  21690.00 ;7959,0,7001  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 132000: 12243.90 Train Score: 3192955.00 ;20676,0,19284 Loss: -0.162068 Valid Score: 7230.00 ;7599,0,7361 TEST Score: \u001b[47m \u001b[37m  20310.00 ;7745,0,7215  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 132300: 11707.55 Train Score: 0.00 ;0,0,0 Loss: -0.162839 Valid Score: 15260.00 ;7351,0,7609 TEST Score: \u001b[47m \u001b[37m  23510.00 ;7477,0,7483  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 132600: 10807.22 Train Score: 0.00 ;0,0,0 Loss: -0.163291 Valid Score: 21330.00 ;7372,0,7588 TEST Score: \u001b[47m \u001b[37m  11610.00 ;7463,0,7497  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 132900: 11019.88 Train Score: 0.00 ;0,0,0 Loss: -0.163512 Valid Score: -2640.00 ;8100,0,6860 TEST Score: \u001b[47m \u001b[37m  6950.00 ;8236,0,6724  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 133200: 7561.10 Train Score: 0.00 ;0,0,0 Loss: -0.163512 Valid Score: 7420.00 ;7762,0,7198 TEST Score: \u001b[47m \u001b[37m  20310.00 ;7913,0,7047  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 133500: 8125.45 Train Score: 2399405.00 ;23783,0,16177 Loss: -0.163059 Valid Score: 3390.00 ;8792,0,6168 TEST Score: \u001b[47m \u001b[37m  18950.00 ;8958,0,6002  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 133800: 7722.83 Train Score: 0.00 ;0,0,0 Loss: -0.162451 Valid Score: -24860.00 ;5655,0,9305 TEST Score: \u001b[47m \u001b[37m  9540.00 ;5767,0,9193  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 134100: 8543.02 Train Score: 0.00 ;0,0,0 Loss: -0.161820 Valid Score: -11530.00 ;9505,0,5455 TEST Score: \u001b[47m \u001b[37m  10320.00 ;9727,0,5233  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 134400: 6781.97 Train Score: 0.00 ;0,0,0 Loss: -0.161387 Valid Score: -22890.00 ;5917,0,9043 TEST Score: \u001b[47m \u001b[37m  20750.00 ;6013,0,8947  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 134700: 8440.00 Train Score: 0.00 ;0,0,0 Loss: -0.161140 Valid Score: 3110.00 ;8145,0,6815 TEST Score: \u001b[47m \u001b[37m  6920.00 ;8309,0,6651  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 135000: 10489.98 Train Score: 2347080.00 ;17428,0,22532 Loss: -0.161337 Valid Score: 8440.00 ;6392,0,8568 TEST Score: \u001b[47m \u001b[37m  12230.00 ;6410,0,8550  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 135300: 8515.37 Train Score: 0.00 ;0,0,0 Loss: -0.161248 Valid Score: -230.00 ;8233,0,6727 TEST Score: \u001b[47m \u001b[37m  -3060.00 ;8371,0,6589  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 135600: 10460.83 Train Score: 0.00 ;0,0,0 Loss: -0.161644 Valid Score: 17510.00 ;6770,0,8190 TEST Score: \u001b[47m \u001b[37m  -13560.00 ;6815,0,8145  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 135900: 12568.80 Train Score: 0.00 ;0,0,0 Loss: -0.162858 Valid Score: 19120.00 ;7617,0,7343 TEST Score: \u001b[47m \u001b[37m  20130.00 ;7748,0,7212  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 136200: 11385.93 Train Score: 0.00 ;0,0,0 Loss: -0.163329 Valid Score: 25610.00 ;7909,0,7051 TEST Score: \u001b[47m \u001b[37m  16410.00 ;8081,0,6879  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 136500: 10842.53 Train Score: 3173785.00 ;20651,0,19309 Loss: -0.163845 Valid Score: 22660.00 ;7623,0,7337 TEST Score: \u001b[47m \u001b[37m  16460.00 ;7753,0,7207  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 136800: 11358.47 Train Score: 0.00 ;0,0,0 Loss: -0.164762 Valid Score: 24680.00 ;7436,0,7524 TEST Score: \u001b[47m \u001b[37m  8250.00 ;7549,0,7411  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 137100: 12929.12 Train Score: 0.00 ;0,0,0 Loss: -0.165074 Valid Score: 19180.00 ;7582,0,7378 TEST Score: \u001b[47m \u001b[37m  12770.00 ;7719,0,7241  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 137400: 14629.72 Train Score: 0.00 ;0,0,0 Loss: -0.165756 Valid Score: 5050.00 ;7523,0,7437 TEST Score: \u001b[47m \u001b[37m  21080.00 ;7674,0,7286  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 137700: 11780.95 Train Score: 0.00 ;0,0,0 Loss: -0.165951 Valid Score: -3450.00 ;7981,0,6979 TEST Score: \u001b[47m \u001b[37m  7350.00 ;8165,0,6795  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 138000: 12891.22 Train Score: 3196265.00 ;21333,0,18627 Loss: -0.166270 Valid Score: -3500.00 ;7885,0,7075 TEST Score: \u001b[47m \u001b[37m  18470.00 ;8057,0,6903  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 138300: 12777.30 Train Score: 0.00 ;0,0,0 Loss: -0.166222 Valid Score: 3470.00 ;8357,0,6603 TEST Score: \u001b[47m \u001b[37m  -1260.00 ;8492,0,6468  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 138600: 10865.43 Train Score: 0.00 ;0,0,0 Loss: -0.165941 Valid Score: 2880.00 ;7784,0,7176 TEST Score: \u001b[47m \u001b[37m  12800.00 ;7928,0,7032  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 138900: 13176.05 Train Score: 0.00 ;0,0,0 Loss: -0.166298 Valid Score: 11130.00 ;7621,0,7339 TEST Score: \u001b[47m \u001b[37m  7360.00 ;7773,0,7187  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 139200: 12194.40 Train Score: 0.00 ;0,0,0 Loss: -0.166226 Valid Score: 410.00 ;7864,0,7096 TEST Score: \u001b[47m \u001b[37m  13430.00 ;8008,0,6952  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 139500: 11858.22 Train Score: 3076985.00 ;19192,0,20768 Loss: -0.166667 Valid Score: 15730.00 ;7034,0,7926 TEST Score: \u001b[47m \u001b[37m  8280.00 ;7138,0,7822  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 139800: 11539.38 Train Score: 0.00 ;0,0,0 Loss: -0.166432 Valid Score: 10840.00 ;7055,0,7905 TEST Score: \u001b[47m \u001b[37m  9800.00 ;7166,0,7794  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 140100: 11955.95 Train Score: 0.00 ;0,0,0 Loss: -0.166608 Valid Score: 9380.00 ;7771,0,7189 TEST Score: \u001b[47m \u001b[37m  13120.00 ;7913,0,7047  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 140400: 12801.17 Train Score: 0.00 ;0,0,0 Loss: -0.166688 Valid Score: 2470.00 ;7758,0,7202 TEST Score: \u001b[47m \u001b[37m  10200.00 ;7916,0,7044  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 140700: 12607.10 Train Score: 0.00 ;0,0,0 Loss: -0.166788 Valid Score: -4500.00 ;8034,0,6926 TEST Score: \u001b[47m \u001b[37m  15020.00 ;8192,0,6768  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 141000: 10674.70 Train Score: 3044550.00 ;21959,0,18001 Loss: -0.167046 Valid Score: -8530.00 ;8163,0,6797 TEST Score: \u001b[47m \u001b[37m  21590.00 ;8308,0,6652  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 141300: 10827.45 Train Score: 0.00 ;0,0,0 Loss: -0.167451 Valid Score: 6730.00 ;7068,0,7892 TEST Score: \u001b[47m \u001b[37m  3900.00 ;7150,0,7810  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 141600: 11031.60 Train Score: 0.00 ;0,0,0 Loss: -0.167806 Valid Score: 20340.00 ;6563,0,8397 TEST Score: \u001b[47m \u001b[37m  -15560.00 ;6619,0,8341  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 141900: 10253.80 Train Score: 0.00 ;0,0,0 Loss: -0.167587 Valid Score: -2520.00 ;9088,0,5872 TEST Score: \u001b[47m \u001b[37m  -9380.00 ;9233,0,5727  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 142200: 9318.78 Train Score: 0.00 ;0,0,0 Loss: -0.167532 Valid Score: 13860.00 ;6293,0,8667 TEST Score: \u001b[47m \u001b[37m  8130.00 ;6315,0,8645  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 142500: 9798.32 Train Score: 2966875.00 ;21838,0,18122 Loss: -0.167178 Valid Score: -24020.00 ;8075,0,6885 TEST Score: \u001b[47m \u001b[37m  5040.00 ;8192,0,6768  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 142800: 9769.80 Train Score: 0.00 ;0,0,0 Loss: -0.167481 Valid Score: -20640.00 ;8491,0,6469 TEST Score: \u001b[47m \u001b[37m  2100.00 ;8599,0,6361  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 143100: 9482.92 Train Score: 0.00 ;0,0,0 Loss: -0.167736 Valid Score: -2980.00 ;5475,0,9485 TEST Score: \u001b[47m \u001b[37m  12720.00 ;5493,0,9467  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 143400: 8457.45 Train Score: 0.00 ;0,0,0 Loss: -0.167318 Valid Score: -2670.00 ;10018,0,4942 TEST Score: \u001b[47m \u001b[37m  -120.00 ;10203,0,4757  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 143700: 7681.17 Train Score: 0.00 ;0,0,0 Loss: -0.166683 Valid Score: -4910.00 ;5334,0,9626 TEST Score: \u001b[47m \u001b[37m  10600.00 ;5313,0,9647  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 144000: 7959.62 Train Score: 2299470.00 ;24057,0,15903 Loss: -0.166563 Valid Score: -17200.00 ;8894,0,6066 TEST Score: \u001b[47m \u001b[37m  -14310.00 ;9068,0,5892  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 144300: 9539.70 Train Score: 0.00 ;0,0,0 Loss: -0.166271 Valid Score: -5320.00 ;7414,0,7546 TEST Score: \u001b[47m \u001b[37m  8510.00 ;7533,0,7427  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 144600: 10174.20 Train Score: 0.00 ;0,0,0 Loss: -0.165581 Valid Score: 4150.00 ;7403,0,7557 TEST Score: \u001b[47m \u001b[37m  -2520.00 ;7524,0,7436  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 144900: 10380.22 Train Score: 0.00 ;0,0,0 Loss: -0.165303 Valid Score: 3290.00 ;7345,0,7615 TEST Score: \u001b[47m \u001b[37m  8290.00 ;7511,0,7449  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 145200: 9641.60 Train Score: 0.00 ;0,0,0 Loss: -0.165331 Valid Score: 5960.00 ;8633,0,6327 TEST Score: \u001b[47m \u001b[37m  -1970.00 ;8771,0,6189  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 145500: 10693.87 Train Score: 3064325.00 ;19477,0,20483 Loss: -0.164649 Valid Score: 25530.00 ;7102,0,7858 TEST Score: \u001b[47m \u001b[37m  5900.00 ;7249,0,7711  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 145800: 10359.57 Train Score: 0.00 ;0,0,0 Loss: -0.164203 Valid Score: 13860.00 ;6795,0,8165 TEST Score: \u001b[47m \u001b[37m  5230.00 ;6909,0,8051  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 146100: 10395.50 Train Score: 0.00 ;0,0,0 Loss: -0.163951 Valid Score: 2080.00 ;8198,0,6762 TEST Score: \u001b[47m \u001b[37m  12080.00 ;8382,0,6578  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 146400: 10109.22 Train Score: 0.00 ;0,0,0 Loss: -0.163941 Valid Score: 9030.00 ;8079,0,6881 TEST Score: \u001b[47m \u001b[37m  17980.00 ;8250,0,6710  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 146700: 9735.15 Train Score: 0.00 ;0,0,0 Loss: -0.163583 Valid Score: 12680.00 ;7084,0,7876 TEST Score: \u001b[47m \u001b[37m  10590.00 ;7190,0,7770  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 147000: 12139.47 Train Score: 3147995.00 ;20156,0,19804 Loss: -0.163471 Valid Score: 7220.00 ;7397,0,7563 TEST Score: \u001b[47m \u001b[37m  3020.00 ;7567,0,7393  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 147300: 11366.85 Train Score: 0.00 ;0,0,0 Loss: -0.163438 Valid Score: 8930.00 ;7680,0,7280 TEST Score: \u001b[47m \u001b[37m  7630.00 ;7812,0,7148  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 147600: 12957.52 Train Score: 0.00 ;0,0,0 Loss: -0.163527 Valid Score: 12320.00 ;7804,0,7156 TEST Score: \u001b[47m \u001b[37m  15940.00 ;7904,0,7056  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 147900: 11938.78 Train Score: 0.00 ;0,0,0 Loss: -0.163230 Valid Score: 28390.00 ;7326,0,7634 TEST Score: \u001b[47m \u001b[37m  5160.00 ;7412,0,7548  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 148200: 10828.00 Train Score: 0.00 ;0,0,0 Loss: -0.163109 Valid Score: 20650.00 ;7277,0,7683 TEST Score: \u001b[47m \u001b[37m  5620.00 ;7333,0,7627  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 148500: 12782.47 Train Score: 3027180.00 ;18843,0,21117 Loss: -0.163104 Valid Score: 17640.00 ;6909,0,8051 TEST Score: \u001b[47m \u001b[37m  1300.00 ;7012,0,7948  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 148800: 12373.40 Train Score: 0.00 ;0,0,0 Loss: -0.163415 Valid Score: 23460.00 ;6893,0,8067 TEST Score: \u001b[47m \u001b[37m  -5660.00 ;7010,0,7950  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 149100: 13158.45 Train Score: 0.00 ;0,0,0 Loss: -0.163229 Valid Score: -3620.00 ;7777,0,7183 TEST Score: \u001b[47m \u001b[37m  21950.00 ;7930,0,7030  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 149400: 12454.62 Train Score: 0.00 ;0,0,0 Loss: -0.163286 Valid Score: -3650.00 ;7955,0,7005 TEST Score: \u001b[47m \u001b[37m  18930.00 ;8118,0,6842  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 149700: 11788.58 Train Score: 0.00 ;0,0,0 Loss: -0.163890 Valid Score: -5050.00 ;8541,0,6419 TEST Score: \u001b[47m \u001b[37m  9610.00 ;8652,0,6308  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 150000: 10877.62 Train Score: 2581915.00 ;17652,0,22308 Loss: -0.164077 Valid Score: 7960.00 ;6459,0,8501 TEST Score: \u001b[47m \u001b[37m  -7220.00 ;6520,0,8440  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 150300: 10026.32 Train Score: 0.00 ;0,0,0 Loss: -0.164284 Valid Score: 15540.00 ;7365,0,7595 TEST Score: \u001b[47m \u001b[37m  -3050.00 ;7474,0,7486  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 150600: 10542.73 Train Score: 0.00 ;0,0,0 Loss: -0.164302 Valid Score: -270.00 ;8750,0,6210 TEST Score: \u001b[47m \u001b[37m  -5710.00 ;8915,0,6045  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 150900: 7019.73 Train Score: 0.00 ;0,0,0 Loss: -0.164125 Valid Score: 29000.00 ;6778,0,8182 TEST Score: \u001b[47m \u001b[37m  1030.00 ;6756,0,8204  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 151200: 9007.38 Train Score: 0.00 ;0,0,0 Loss: -0.164056 Valid Score: -470.00 ;8449,0,6511 TEST Score: \u001b[47m \u001b[37m  -13240.00 ;8599,0,6361  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 151500: 8286.45 Train Score: 2680385.00 ;17941,0,22019 Loss: -0.163932 Valid Score: 8600.00 ;6560,0,8400 TEST Score: \u001b[47m \u001b[37m  6700.00 ;6628,0,8332  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 151800: 10026.88 Train Score: 0.00 ;0,0,0 Loss: -0.163805 Valid Score: -11480.00 ;8945,0,6015 TEST Score: \u001b[47m \u001b[37m  6530.00 ;9027,0,5933  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 152100: 9825.92 Train Score: 0.00 ;0,0,0 Loss: -0.163738 Valid Score: 8910.00 ;6191,0,8769 TEST Score: \u001b[47m \u001b[37m  -5690.00 ;6295,0,8665  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 152400: 9152.30 Train Score: 0.00 ;0,0,0 Loss: -0.163555 Valid Score: 14230.00 ;9041,0,5919 TEST Score: \u001b[47m \u001b[37m  18550.00 ;9167,0,5793  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 152700: 8640.52 Train Score: 0.00 ;0,0,0 Loss: -0.163040 Valid Score: 26000.00 ;6662,0,8298 TEST Score: \u001b[47m \u001b[37m  -1880.00 ;6727,0,8233  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 153000: 9108.65 Train Score: 2935325.00 ;21400,0,18560 Loss: -0.162374 Valid Score: 1820.00 ;7869,0,7091 TEST Score: \u001b[47m \u001b[37m  7520.00 ;8005,0,6955  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 153300: 10780.38 Train Score: 0.00 ;0,0,0 Loss: -0.162527 Valid Score: 23290.00 ;6996,0,7964 TEST Score: \u001b[47m \u001b[37m  6070.00 ;7096,0,7864  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 153600: 11759.68 Train Score: 0.00 ;0,0,0 Loss: -0.162501 Valid Score: 5350.00 ;8023,0,6937 TEST Score: \u001b[47m \u001b[37m  28090.00 ;8190,0,6770  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 153900: 11741.58 Train Score: 0.00 ;0,0,0 Loss: -0.162728 Valid Score: 2350.00 ;7112,0,7848 TEST Score: \u001b[47m \u001b[37m  4070.00 ;7247,0,7713  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 154200: 10129.18 Train Score: 0.00 ;0,0,0 Loss: -0.162964 Valid Score: -290.00 ;7840,0,7120 TEST Score: \u001b[47m \u001b[37m  13050.00 ;8015,0,6945  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 154500: 13170.75 Train Score: 3281060.00 ;20433,0,19527 Loss: -0.163117 Valid Score: 20280.00 ;7536,0,7424 TEST Score: \u001b[47m \u001b[37m  13070.00 ;7663,0,7297  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 154800: 12049.92 Train Score: 0.00 ;0,0,0 Loss: -0.163016 Valid Score: 7880.00 ;6995,0,7965 TEST Score: \u001b[47m \u001b[37m  4880.00 ;7064,0,7896  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 155100: 12077.45 Train Score: 0.00 ;0,0,0 Loss: -0.163190 Valid Score: 8490.00 ;7282,0,7678 TEST Score: \u001b[47m \u001b[37m  21980.00 ;7387,0,7573  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 155400: 11115.55 Train Score: 0.00 ;0,0,0 Loss: -0.163105 Valid Score: 24910.00 ;7566,0,7394 TEST Score: \u001b[47m \u001b[37m  11640.00 ;7695,0,7265  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 155700: 11542.28 Train Score: 0.00 ;0,0,0 Loss: -0.163317 Valid Score: -3010.00 ;7747,0,7213 TEST Score: \u001b[47m \u001b[37m  7140.00 ;7904,0,7056  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 156000: 12044.50 Train Score: 3255565.00 ;21297,0,18663 Loss: -0.163721 Valid Score: -130.00 ;7905,0,7055 TEST Score: \u001b[47m \u001b[37m  19100.00 ;8032,0,6928  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 156300: 12962.53 Train Score: 0.00 ;0,0,0 Loss: -0.164317 Valid Score: -13080.00 ;8016,0,6944 TEST Score: \u001b[47m \u001b[37m  31300.00 ;8134,0,6826  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 156600: 12316.13 Train Score: 0.00 ;0,0,0 Loss: -0.165049 Valid Score: -10020.00 ;7643,0,7317 TEST Score: \u001b[47m \u001b[37m  25270.00 ;7789,0,7171  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 156900: 10863.75 Train Score: 0.00 ;0,0,0 Loss: -0.165379 Valid Score: -6260.00 ;7923,0,7037 TEST Score: \u001b[47m \u001b[37m  14410.00 ;8068,0,6892  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 157200: 10970.50 Train Score: 0.00 ;0,0,0 Loss: -0.165805 Valid Score: 7620.00 ;6843,0,8117 TEST Score: \u001b[47m \u001b[37m  740.00 ;6948,0,8012  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 157500: 13454.82 Train Score: 3084900.00 ;18917,0,21043 Loss: -0.166782 Valid Score: 9010.00 ;6921,0,8039 TEST Score: \u001b[47m \u001b[37m  7600.00 ;7025,0,7935  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 157800: 12162.50 Train Score: 0.00 ;0,0,0 Loss: -0.167161 Valid Score: 2220.00 ;7069,0,7891 TEST Score: \u001b[47m \u001b[37m  5280.00 ;7179,0,7781  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 158100: 11862.07 Train Score: 0.00 ;0,0,0 Loss: -0.167384 Valid Score: 4960.00 ;7718,0,7242 TEST Score: \u001b[47m \u001b[37m  23430.00 ;7856,0,7104  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 158400: 11055.23 Train Score: 0.00 ;0,0,0 Loss: -0.167693 Valid Score: 790.00 ;7905,0,7055 TEST Score: \u001b[47m \u001b[37m  29570.00 ;8018,0,6942  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 158700: 11796.62 Train Score: 0.00 ;0,0,0 Loss: -0.168304 Valid Score: 2080.00 ;8549,0,6411 TEST Score: \u001b[47m \u001b[37m  24770.00 ;8667,0,6293  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 159000: 6532.52 Train Score: 1324480.00 ;13066,0,26894 Loss: -0.167894 Valid Score: -10660.00 ;4862,0,10098 TEST Score: \u001b[47m \u001b[37m  30620.00 ;4859,0,10101  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 159300: 8153.25 Train Score: 0.00 ;0,0,0 Loss: -0.167017 Valid Score: -8400.00 ;9680,0,5280 TEST Score: \u001b[47m \u001b[37m  -2540.00 ;9887,0,5073  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 159600: 7206.90 Train Score: 0.00 ;0,0,0 Loss: -0.166282 Valid Score: -7320.00 ;8110,0,6850 TEST Score: \u001b[47m \u001b[37m  22250.00 ;8278,0,6682  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 159900: 7240.27 Train Score: 0.00 ;0,0,0 Loss: -0.166124 Valid Score: 4770.00 ;4644,0,10316 TEST Score: \u001b[47m \u001b[37m  41010.00 ;4666,0,10294  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 160200: 5988.43 Train Score: 0.00 ;0,0,0 Loss: -0.165056 Valid Score: 26260.00 ;8030,0,6930 TEST Score: \u001b[47m \u001b[37m  7180.00 ;8206,0,6754  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 160500: 7726.62 Train Score: 2344995.00 ;23568,0,16392 Loss: -0.164610 Valid Score: -7530.00 ;8713,0,6247 TEST Score: \u001b[47m \u001b[37m  -5580.00 ;8891,0,6069  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 160800: 9359.97 Train Score: 0.00 ;0,0,0 Loss: -0.164348 Valid Score: 17270.00 ;5627,0,9333 TEST Score: \u001b[47m \u001b[37m  24700.00 ;5646,0,9314  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 161100: 7436.48 Train Score: 0.00 ;0,0,0 Loss: -0.163953 Valid Score: -1610.00 ;8435,0,6525 TEST Score: \u001b[47m \u001b[37m  18040.00 ;8601,0,6359  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 161400: 7724.78 Train Score: 0.00 ;0,0,0 Loss: -0.163373 Valid Score: 8790.00 ;7551,0,7409 TEST Score: \u001b[47m \u001b[37m  7210.00 ;7642,0,7318  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 161700: 9057.02 Train Score: 0.00 ;0,0,0 Loss: -0.162906 Valid Score: -1020.00 ;8030,0,6930 TEST Score: \u001b[47m \u001b[37m  11030.00 ;8142,0,6818  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 162000: 10120.65 Train Score: 2892800.00 ;18733,0,21227 Loss: -0.162723 Valid Score: -7710.00 ;6838,0,8122 TEST Score: \u001b[47m \u001b[37m  1050.00 ;6985,0,7975  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 162300: 10110.33 Train Score: 0.00 ;0,0,0 Loss: -0.162541 Valid Score: 11400.00 ;8794,0,6166 TEST Score: \u001b[47m \u001b[37m  22960.00 ;8910,0,6050  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 162600: 9738.90 Train Score: 0.00 ;0,0,0 Loss: -0.162505 Valid Score: -12400.00 ;7621,0,7339 TEST Score: \u001b[47m \u001b[37m  16920.00 ;7714,0,7246  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 162900: 10954.60 Train Score: 0.00 ;0,0,0 Loss: -0.162687 Valid Score: 2770.00 ;6729,0,8231 TEST Score: \u001b[47m \u001b[37m  -3220.00 ;6826,0,8134  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 163200: 10119.07 Train Score: 0.00 ;0,0,0 Loss: -0.162838 Valid Score: -10510.00 ;8880,0,6080 TEST Score: \u001b[47m \u001b[37m  1260.00 ;8991,0,5969  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 163500: 10879.60 Train Score: 2955970.00 ;18709,0,21251 Loss: -0.163386 Valid Score: -610.00 ;6834,0,8126 TEST Score: \u001b[47m \u001b[37m  -6050.00 ;6915,0,8045  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 163800: 11782.87 Train Score: 0.00 ;0,0,0 Loss: -0.164001 Valid Score: 13060.00 ;7300,0,7660 TEST Score: \u001b[47m \u001b[37m  15610.00 ;7429,0,7531  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 164100: 10530.25 Train Score: 0.00 ;0,0,0 Loss: -0.164532 Valid Score: -6620.00 ;7964,0,6996 TEST Score: \u001b[47m \u001b[37m  17040.00 ;8075,0,6885  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 164400: 12066.50 Train Score: 0.00 ;0,0,0 Loss: -0.165705 Valid Score: -7930.00 ;7913,0,7047 TEST Score: \u001b[47m \u001b[37m  16260.00 ;8042,0,6918  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 164700: 11791.65 Train Score: 0.00 ;0,0,0 Loss: -0.166432 Valid Score: 10150.00 ;6811,0,8149 TEST Score: \u001b[47m \u001b[37m  3180.00 ;6921,0,8039  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 165000: 11152.80 Train Score: 3072190.00 ;22035,0,17925 Loss: -0.166414 Valid Score: -20050.00 ;8211,0,6749 TEST Score: \u001b[47m \u001b[37m  22970.00 ;8316,0,6644  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 165300: 11995.28 Train Score: 0.00 ;0,0,0 Loss: -0.167003 Valid Score: 2840.00 ;7711,0,7249 TEST Score: \u001b[47m \u001b[37m  4590.00 ;7825,0,7135  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 165600: 12118.85 Train Score: 0.00 ;0,0,0 Loss: -0.167273 Valid Score: 2420.00 ;6706,0,8254 TEST Score: \u001b[47m \u001b[37m  -3170.00 ;6853,0,8107  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 165900: 10977.32 Train Score: 0.00 ;0,0,0 Loss: -0.166880 Valid Score: 8130.00 ;7484,0,7476 TEST Score: \u001b[47m \u001b[37m  11310.00 ;7612,0,7348  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 166200: 12162.75 Train Score: 0.00 ;0,0,0 Loss: -0.166907 Valid Score: -24610.00 ;8428,0,6532 TEST Score: \u001b[47m \u001b[37m  24020.00 ;8564,0,6396  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 166500: 10555.72 Train Score: 3190620.00 ;19432,0,20528 Loss: -0.166800 Valid Score: 4700.00 ;7092,0,7868 TEST Score: \u001b[47m \u001b[37m  4360.00 ;7259,0,7701  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 166800: 11651.40 Train Score: 0.00 ;0,0,0 Loss: -0.166277 Valid Score: 4980.00 ;7484,0,7476 TEST Score: \u001b[47m \u001b[37m  12660.00 ;7609,0,7351  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 167100: 11718.85 Train Score: 0.00 ;0,0,0 Loss: -0.166330 Valid Score: -16830.00 ;8432,0,6528 TEST Score: \u001b[47m \u001b[37m  9550.00 ;8564,0,6396  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 167400: 11574.25 Train Score: 0.00 ;0,0,0 Loss: -0.166470 Valid Score: -2770.00 ;7618,0,7342 TEST Score: \u001b[47m \u001b[37m  19610.00 ;7720,0,7240  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 167700: 11876.55 Train Score: 0.00 ;0,0,0 Loss: -0.166721 Valid Score: 9670.00 ;7152,0,7808 TEST Score: \u001b[47m \u001b[37m  18240.00 ;7297,0,7663  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 168000: 12798.80 Train Score: 3219380.00 ;19405,0,20555 Loss: -0.166581 Valid Score: -2660.00 ;7084,0,7876 TEST Score: \u001b[47m \u001b[37m  13610.00 ;7212,0,7748  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 168300: 12102.60 Train Score: 0.00 ;0,0,0 Loss: -0.166450 Valid Score: -24400.00 ;8148,0,6812 TEST Score: \u001b[47m \u001b[37m  9980.00 ;8293,0,6667  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 168600: 10531.32 Train Score: 0.00 ;0,0,0 Loss: -0.166364 Valid Score: 10790.00 ;7388,0,7572 TEST Score: \u001b[47m \u001b[37m  12370.00 ;7496,0,7464  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 168900: 11163.57 Train Score: 0.00 ;0,0,0 Loss: -0.165990 Valid Score: -10570.00 ;6912,0,8048 TEST Score: \u001b[47m \u001b[37m  14420.00 ;7049,0,7911  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 169200: 12575.25 Train Score: 0.00 ;0,0,0 Loss: -0.166100 Valid Score: -5630.00 ;6814,0,8146 TEST Score: \u001b[47m \u001b[37m  7600.00 ;6952,0,8008  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 169500: 12102.92 Train Score: 3351920.00 ;20684,0,19276 Loss: -0.165889 Valid Score: -2180.00 ;7621,0,7339 TEST Score: \u001b[47m \u001b[37m  27110.00 ;7734,0,7226  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 169800: 13063.33 Train Score: 0.00 ;0,0,0 Loss: -0.166090 Valid Score: 890.00 ;7912,0,7048 TEST Score: \u001b[47m \u001b[37m  20100.00 ;8039,0,6921  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 170100: 12101.57 Train Score: 0.00 ;0,0,0 Loss: -0.165820 Valid Score: 9510.00 ;7492,0,7468 TEST Score: \u001b[47m \u001b[37m  15470.00 ;7571,0,7389  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 170400: 12371.62 Train Score: 0.00 ;0,0,0 Loss: -0.165938 Valid Score: 3750.00 ;7569,0,7391 TEST Score: \u001b[47m \u001b[37m  9110.00 ;7665,0,7295  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 170700: 11174.97 Train Score: 0.00 ;0,0,0 Loss: -0.165920 Valid Score: 2350.00 ;7350,0,7610 TEST Score: \u001b[47m \u001b[37m  13320.00 ;7484,0,7476  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 171000: 11932.88 Train Score: 3201850.00 ;19119,0,20841 Loss: -0.166174 Valid Score: -650.00 ;6982,0,7978 TEST Score: \u001b[47m \u001b[37m  7990.00 ;7107,0,7853  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 171300: 11899.10 Train Score: 0.00 ;0,0,0 Loss: -0.166040 Valid Score: 11760.00 ;7288,0,7672 TEST Score: \u001b[47m \u001b[37m  11500.00 ;7393,0,7567  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 171600: 12004.42 Train Score: 0.00 ;0,0,0 Loss: -0.166307 Valid Score: 14930.00 ;7812,0,7148 TEST Score: \u001b[47m \u001b[37m  29420.00 ;7967,0,6993  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 171900: 14185.75 Train Score: 0.00 ;0,0,0 Loss: -0.166892 Valid Score: 7190.00 ;7524,0,7436 TEST Score: \u001b[47m \u001b[37m  17380.00 ;7659,0,7301  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 172200: 12101.78 Train Score: 0.00 ;0,0,0 Loss: -0.167369 Valid Score: 2120.00 ;7333,0,7627 TEST Score: \u001b[47m \u001b[37m  7510.00 ;7473,0,7487  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 172500: 12176.73 Train Score: 3399100.00 ;20582,0,19378 Loss: -0.167680 Valid Score: 12890.00 ;7606,0,7354 TEST Score: \u001b[47m \u001b[37m  18880.00 ;7718,0,7242  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 172800: 12023.30 Train Score: 0.00 ;0,0,0 Loss: -0.167815 Valid Score: 16010.00 ;7585,0,7375 TEST Score: \u001b[47m \u001b[37m  15640.00 ;7711,0,7249  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 173100: 11006.13 Train Score: 0.00 ;0,0,0 Loss: -0.167982 Valid Score: -18790.00 ;8981,0,5979 TEST Score: \u001b[47m \u001b[37m  16000.00 ;9137,0,5823  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 173400: 9684.95 Train Score: 0.00 ;0,0,0 Loss: -0.168103 Valid Score: 3030.00 ;8016,0,6944 TEST Score: \u001b[47m \u001b[37m  36160.00 ;8166,0,6794  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 173700: 12116.25 Train Score: 0.00 ;0,0,0 Loss: -0.168677 Valid Score: 10090.00 ;6975,0,7985 TEST Score: \u001b[47m \u001b[37m  12730.00 ;7096,0,7864  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 174000: 10962.38 Train Score: 3193360.00 ;19350,0,20610 Loss: -0.169134 Valid Score: 15250.00 ;7080,0,7880 TEST Score: \u001b[47m \u001b[37m  3660.00 ;7186,0,7774  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 174300: 10802.80 Train Score: 0.00 ;0,0,0 Loss: -0.169536 Valid Score: 13460.00 ;6841,0,8119 TEST Score: \u001b[47m \u001b[37m  -1470.00 ;6925,0,8035  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 174600: 11673.42 Train Score: 0.00 ;0,0,0 Loss: -0.169701 Valid Score: 10760.00 ;7075,0,7885 TEST Score: \u001b[47m \u001b[37m  5210.00 ;7185,0,7775  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 174900: 12065.27 Train Score: 0.00 ;0,0,0 Loss: -0.169860 Valid Score: 22120.00 ;7813,0,7147 TEST Score: \u001b[47m \u001b[37m  22410.00 ;7891,0,7069  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 175200: 11707.93 Train Score: 0.00 ;0,0,0 Loss: -0.169816 Valid Score: 18910.00 ;8367,0,6593 TEST Score: \u001b[47m \u001b[37m  15070.00 ;8506,0,6454  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 175500: 10051.72 Train Score: 3310485.00 ;19999,0,19961 Loss: -0.169705 Valid Score: 1560.00 ;7349,0,7611 TEST Score: \u001b[47m \u001b[37m  -1750.00 ;7440,0,7520  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 175800: 12333.67 Train Score: 0.00 ;0,0,0 Loss: -0.170024 Valid Score: 1390.00 ;6811,0,8149 TEST Score: \u001b[47m \u001b[37m  -9000.00 ;6944,0,8016  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 176100: 11246.68 Train Score: 0.00 ;0,0,0 Loss: -0.170188 Valid Score: 19760.00 ;7714,0,7246 TEST Score: \u001b[47m \u001b[37m  6960.00 ;7841,0,7119  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 176400: 12078.02 Train Score: 0.00 ;0,0,0 Loss: -0.170173 Valid Score: -13020.00 ;8272,0,6688 TEST Score: \u001b[47m \u001b[37m  23110.00 ;8391,0,6569  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 176700: 12933.30 Train Score: 0.00 ;0,0,0 Loss: -0.170370 Valid Score: -800.00 ;7762,0,7198 TEST Score: \u001b[47m \u001b[37m  27070.00 ;7868,0,7092  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 177000: 12130.80 Train Score: 3269915.00 ;19412,0,20548 Loss: -0.170602 Valid Score: 14310.00 ;7101,0,7859 TEST Score: \u001b[47m \u001b[37m  1660.00 ;7217,0,7743  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 177300: 11805.23 Train Score: 0.00 ;0,0,0 Loss: -0.170461 Valid Score: 15950.00 ;7063,0,7897 TEST Score: \u001b[47m \u001b[37m  -3230.00 ;7172,0,7788  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 177600: 10639.27 Train Score: 0.00 ;0,0,0 Loss: -0.170350 Valid Score: -6360.00 ;8911,0,6049 TEST Score: \u001b[47m \u001b[37m  16900.00 ;9049,0,5911  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 177900: 10238.18 Train Score: 0.00 ;0,0,0 Loss: -0.170143 Valid Score: 15290.00 ;6850,0,8110 TEST Score: \u001b[47m \u001b[37m  -3640.00 ;6942,0,8018  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 178200: 11157.43 Train Score: 0.00 ;0,0,0 Loss: -0.170126 Valid Score: 12260.00 ;6919,0,8041 TEST Score: \u001b[47m \u001b[37m  7010.00 ;6989,0,7971  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 178500: 11369.33 Train Score: 2468640.00 ;23953,0,16007 Loss: -0.169992 Valid Score: 840.00 ;8914,0,6046 TEST Score: \u001b[47m \u001b[37m  880.00 ;9064,0,5896  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 178800: 10398.87 Train Score: 0.00 ;0,0,0 Loss: -0.169715 Valid Score: -1750.00 ;6168,0,8792 TEST Score: \u001b[47m \u001b[37m  6610.00 ;6217,0,8743  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 179100: 9855.48 Train Score: 0.00 ;0,0,0 Loss: -0.169429 Valid Score: -2760.00 ;7799,0,7161 TEST Score: \u001b[47m \u001b[37m  21640.00 ;7925,0,7035  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 179400: 10340.93 Train Score: 0.00 ;0,0,0 Loss: -0.169454 Valid Score: 6430.00 ;8450,0,6510 TEST Score: \u001b[47m \u001b[37m  17050.00 ;8585,0,6375  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 179700: 10430.73 Train Score: 0.00 ;0,0,0 Loss: -0.169230 Valid Score: 16080.00 ;6963,0,7997 TEST Score: \u001b[47m \u001b[37m  3640.00 ;7084,0,7876  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 180000: 11558.68 Train Score: 3290600.00 ;20391,0,19569 Loss: -0.169440 Valid Score: 16610.00 ;7505,0,7455 TEST Score: \u001b[47m \u001b[37m  11740.00 ;7642,0,7318  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 180300: 12086.82 Train Score: 0.00 ;0,0,0 Loss: -0.170042 Valid Score: 13210.00 ;7625,0,7335 TEST Score: \u001b[47m \u001b[37m  17340.00 ;7734,0,7226  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 180600: 13013.47 Train Score: 0.00 ;0,0,0 Loss: -0.170799 Valid Score: -2890.00 ;8460,0,6500 TEST Score: \u001b[47m \u001b[37m  16540.00 ;8584,0,6376  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 180900: 12197.20 Train Score: 0.00 ;0,0,0 Loss: -0.171287 Valid Score: 12090.00 ;7399,0,7561 TEST Score: \u001b[47m \u001b[37m  15260.00 ;7510,0,7450  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 181200: 13760.35 Train Score: 0.00 ;0,0,0 Loss: -0.171674 Valid Score: 11830.00 ;7205,0,7755 TEST Score: \u001b[47m \u001b[37m  2190.00 ;7324,0,7636  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 181500: 12585.15 Train Score: 3232555.00 ;21886,0,18074 Loss: -0.172076 Valid Score: -12250.00 ;8152,0,6808 TEST Score: \u001b[47m \u001b[37m  22730.00 ;8282,0,6678  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 181800: 12864.30 Train Score: 0.00 ;0,0,0 Loss: -0.172302 Valid Score: -130.00 ;7734,0,7226 TEST Score: \u001b[47m \u001b[37m  5090.00 ;7844,0,7116  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 182100: 11918.35 Train Score: 0.00 ;0,0,0 Loss: -0.173179 Valid Score: 18420.00 ;7120,0,7840 TEST Score: \u001b[47m \u001b[37m  11180.00 ;7229,0,7731  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 182400: 13425.83 Train Score: 0.00 ;0,0,0 Loss: -0.174048 Valid Score: 18060.00 ;7136,0,7824 TEST Score: \u001b[47m \u001b[37m  6080.00 ;7233,0,7727  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 182700: 12612.65 Train Score: 0.00 ;0,0,0 Loss: -0.174927 Valid Score: 18920.00 ;7529,0,7431 TEST Score: \u001b[47m \u001b[37m  8020.00 ;7662,0,7298  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 183000: 11736.88 Train Score: 3092910.00 ;22240,0,17720 Loss: -0.175396 Valid Score: -6660.00 ;8269,0,6691 TEST Score: \u001b[47m \u001b[37m  24900.00 ;8432,0,6528  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 183300: 12718.60 Train Score: 0.00 ;0,0,0 Loss: -0.175449 Valid Score: 10270.00 ;7560,0,7400 TEST Score: \u001b[47m \u001b[37m  13010.00 ;7669,0,7291  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 183600: 11944.53 Train Score: 0.00 ;0,0,0 Loss: -0.175589 Valid Score: 7150.00 ;7135,0,7825 TEST Score: \u001b[47m \u001b[37m  1740.00 ;7278,0,7682  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 183900: 11522.67 Train Score: 0.00 ;0,0,0 Loss: -0.175445 Valid Score: 11650.00 ;7569,0,7391 TEST Score: \u001b[47m \u001b[37m  15150.00 ;7722,0,7238  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 184200: 11469.28 Train Score: 0.00 ;0,0,0 Loss: -0.175552 Valid Score: 2960.00 ;7504,0,7456 TEST Score: \u001b[47m \u001b[37m  16400.00 ;7625,0,7335  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 184500: 11795.32 Train Score: 3168930.00 ;22027,0,17933 Loss: -0.175374 Valid Score: -6210.00 ;8211,0,6749 TEST Score: \u001b[47m \u001b[37m  33500.00 ;8341,0,6619  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 184800: 11371.05 Train Score: 0.00 ;0,0,0 Loss: -0.175140 Valid Score: 9010.00 ;7942,0,7018 TEST Score: \u001b[47m \u001b[37m  29160.00 ;8070,0,6890  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 185100: 12021.57 Train Score: 0.00 ;0,0,0 Loss: -0.174942 Valid Score: -570.00 ;6728,0,8232 TEST Score: \u001b[47m \u001b[37m  -5350.00 ;6786,0,8174  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 185400: 11362.25 Train Score: 0.00 ;0,0,0 Loss: -0.174973 Valid Score: -2170.00 ;7213,0,7747 TEST Score: \u001b[47m \u001b[37m  1500.00 ;7284,0,7676  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 185700: 13139.22 Train Score: 0.00 ;0,0,0 Loss: -0.174949 Valid Score: 3820.00 ;7716,0,7244 TEST Score: \u001b[47m \u001b[37m  28040.00 ;7850,0,7110  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 186000: 13121.68 Train Score: 3441275.00 ;20946,0,19014 Loss: -0.175096 Valid Score: 8290.00 ;7737,0,7223 TEST Score: \u001b[47m \u001b[37m  25850.00 ;7885,0,7075  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 186300: 12903.38 Train Score: 0.00 ;0,0,0 Loss: -0.175430 Valid Score: -2180.00 ;8074,0,6886 TEST Score: \u001b[47m \u001b[37m  20990.00 ;8231,0,6729  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 186600: 13185.62 Train Score: 0.00 ;0,0,0 Loss: -0.175381 Valid Score: 2050.00 ;7931,0,7029 TEST Score: \u001b[47m \u001b[37m  16330.00 ;8075,0,6885  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 186900: 13965.22 Train Score: 0.00 ;0,0,0 Loss: -0.175786 Valid Score: -580.00 ;7861,0,7099 TEST Score: \u001b[47m \u001b[37m  31410.00 ;8020,0,6940  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 187200: 12485.33 Train Score: 0.00 ;0,0,0 Loss: -0.175859 Valid Score: 7390.00 ;7487,0,7473 TEST Score: \u001b[47m \u001b[37m  24090.00 ;7608,0,7352  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 187500: 14398.75 Train Score: 3430315.00 ;21220,0,18740 Loss: -0.175953 Valid Score: 3140.00 ;7878,0,7082 TEST Score: \u001b[47m \u001b[37m  25750.00 ;8002,0,6958  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 187800: 12420.75 Train Score: 0.00 ;0,0,0 Loss: -0.176212 Valid Score: -5970.00 ;8332,0,6628 TEST Score: \u001b[47m \u001b[37m  11110.00 ;8500,0,6460  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 188100: 12729.08 Train Score: 0.00 ;0,0,0 Loss: -0.176660 Valid Score: -1720.00 ;7937,0,7023 TEST Score: \u001b[47m \u001b[37m  9730.00 ;8068,0,6892  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 188400: 12144.27 Train Score: 0.00 ;0,0,0 Loss: -0.176628 Valid Score: 810.00 ;8078,0,6882 TEST Score: \u001b[47m \u001b[37m  22550.00 ;8230,0,6730  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 188700: 12567.92 Train Score: 0.00 ;0,0,0 Loss: -0.176887 Valid Score: 11140.00 ;8464,0,6496 TEST Score: \u001b[47m \u001b[37m  14250.00 ;8639,0,6321  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 189000: 11917.38 Train Score: 1971450.00 ;15496,0,24464 Loss: -0.177553 Valid Score: 3620.00 ;5723,0,9237 TEST Score: \u001b[47m \u001b[37m  2440.00 ;5724,0,9236  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 189300: 9492.42 Train Score: 0.00 ;0,0,0 Loss: -0.177711 Valid Score: -7370.00 ;9150,0,5810 TEST Score: \u001b[47m \u001b[37m  3080.00 ;9329,0,5631  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 189600: 10756.00 Train Score: 0.00 ;0,0,0 Loss: -0.178208 Valid Score: 12230.00 ;6557,0,8403 TEST Score: \u001b[47m \u001b[37m  -22230.00 ;6647,0,8313  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 189900: 9940.25 Train Score: 0.00 ;0,0,0 Loss: -0.178452 Valid Score: -7140.00 ;8064,0,6896 TEST Score: \u001b[47m \u001b[37m  16540.00 ;8218,0,6742  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 190200: 10241.68 Train Score: 0.00 ;0,0,0 Loss: -0.179084 Valid Score: 11010.00 ;7077,0,7883 TEST Score: \u001b[47m \u001b[37m  110.00 ;7206,0,7754  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 190500: 8694.98 Train Score: 3154305.00 ;20340,0,19620 Loss: -0.179555 Valid Score: 3290.00 ;7493,0,7467 TEST Score: \u001b[47m \u001b[37m  23910.00 ;7666,0,7294  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 190800: 8754.77 Train Score: 0.00 ;0,0,0 Loss: -0.179417 Valid Score: 5480.00 ;5878,0,9082 TEST Score: \u001b[47m \u001b[37m  -180.00 ;5944,0,9016  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 191100: 9791.68 Train Score: 0.00 ;0,0,0 Loss: -0.179614 Valid Score: 33890.00 ;8578,0,6382 TEST Score: \u001b[47m \u001b[37m  20850.00 ;8797,0,6163  \u001b[0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 191400: 9805.22 Train Score: 0.00 ;0,0,0 Loss: -0.179700 Valid Score: 6010.00 ;7032,0,7928 TEST Score: \u001b[47m \u001b[37m  15860.00 ;7156,0,7804  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 191700: 10000.37 Train Score: 0.00 ;0,0,0 Loss: -0.179978 Valid Score: 14870.00 ;7652,0,7308 TEST Score: \u001b[47m \u001b[37m  -5630.00 ;7751,0,7209  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 192000: 11682.39 Train Score: 2966009.00 ;21663,1,18296 Loss: -0.180021 Valid Score: -2040.00 ;8009,0,6951 TEST Score: \u001b[47m \u001b[37m  -7700.00 ;8147,0,6813  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 192300: 10837.70 Train Score: 0.00 ;0,0,0 Loss: -0.179841 Valid Score: 1530.00 ;6771,0,8189 TEST Score: \u001b[47m \u001b[37m  -26810.00 ;6831,0,8129  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 192600: 11100.85 Train Score: 0.00 ;0,0,0 Loss: -0.180058 Valid Score: 8150.00 ;7736,0,7224 TEST Score: \u001b[47m \u001b[37m  7920.00 ;7884,0,7076  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 192900: 11185.71 Train Score: 0.00 ;0,0,0 Loss: -0.180131 Valid Score: 2300.00 ;8476,0,6484 TEST Score: \u001b[47m \u001b[37m  22720.00 ;8623,0,6337  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 193200: 10914.28 Train Score: 0.00 ;0,0,0 Loss: -0.180239 Valid Score: 10760.00 ;7292,0,7668 TEST Score: \u001b[47m \u001b[37m  7570.00 ;7405,0,7555  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 193500: 9437.41 Train Score: 3036759.00 ;21274,1,18685 Loss: -0.180089 Valid Score: -9910.00 ;7869,0,7091 TEST Score: \u001b[47m \u001b[37m  6890.00 ;8015,0,6945  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 193800: 8563.01 Train Score: 0.00 ;0,0,0 Loss: -0.179576 Valid Score: 7710.00 ;6155,0,8805 TEST Score: \u001b[47m \u001b[37m  19730.00 ;6127,0,8833  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 194100: 9104.00 Train Score: 0.00 ;0,0,0 Loss: -0.179113 Valid Score: 19860.00 ;9258,0,5702 TEST Score: \u001b[47m \u001b[37m  -5200.00 ;9421,0,5539  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 194400: 8439.12 Train Score: 0.00 ;0,0,0 Loss: -0.178035 Valid Score: 8020.00 ;5781,0,9179 TEST Score: \u001b[47m \u001b[37m  12150.00 ;5746,0,9214  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 194700: 9867.63 Train Score: 0.00 ;0,0,0 Loss: -0.177815 Valid Score: 16310.00 ;8975,0,5985 TEST Score: \u001b[47m \u001b[37m  17950.00 ;9182,0,5778  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 195000: 9038.13 Train Score: 2385770.00 ;16823,0,23137 Loss: -0.177521 Valid Score: 9830.00 ;6153,0,8807 TEST Score: \u001b[47m \u001b[37m  -2910.00 ;6188,0,8772  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 195300: 7522.45 Train Score: 0.00 ;0,0,0 Loss: -0.176829 Valid Score: 1530.00 ;8017,0,6943 TEST Score: \u001b[47m \u001b[37m  31620.00 ;8155,0,6805  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 195600: 9539.62 Train Score: 0.00 ;0,0,0 Loss: -0.176564 Valid Score: 14020.00 ;6426,0,8534 TEST Score: \u001b[47m \u001b[37m  -200.00 ;6475,0,8485  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 195900: 9319.68 Train Score: 0.00 ;0,0,0 Loss: -0.176335 Valid Score: -3220.00 ;8253,0,6707 TEST Score: \u001b[47m \u001b[37m  31490.00 ;8419,0,6541  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 196200: 9779.15 Train Score: 0.00 ;0,0,0 Loss: -0.176007 Valid Score: 24440.00 ;6556,0,8404 TEST Score: \u001b[47m \u001b[37m  -1760.00 ;6578,0,8382  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 196500: 8657.72 Train Score: 2822340.00 ;22612,0,17348 Loss: -0.175762 Valid Score: -9880.00 ;8369,0,6591 TEST Score: \u001b[47m \u001b[37m  11980.00 ;8549,0,6411  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 196800: 8604.43 Train Score: 0.00 ;0,0,0 Loss: -0.175362 Valid Score: 1980.00 ;8077,0,6883 TEST Score: \u001b[47m \u001b[37m  9860.00 ;8213,0,6747  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 197100: 9298.75 Train Score: 0.00 ;0,0,0 Loss: -0.174804 Valid Score: 13470.00 ;7469,0,7491 TEST Score: \u001b[47m \u001b[37m  8600.00 ;7598,0,7362  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 197400: 9891.57 Train Score: 0.00 ;0,0,0 Loss: -0.174117 Valid Score: 5190.00 ;7720,0,7240 TEST Score: \u001b[47m \u001b[37m  -2630.00 ;7866,0,7094  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 197700: 10898.63 Train Score: 0.00 ;0,0,0 Loss: -0.173936 Valid Score: -10690.00 ;8308,0,6652 TEST Score: \u001b[47m \u001b[37m  4830.00 ;8456,0,6504  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 198000: 11736.98 Train Score: 2670645.00 ;17795,0,22165 Loss: -0.173826 Valid Score: 7960.00 ;6531,0,8429 TEST Score: \u001b[47m \u001b[37m  20.00 ;6612,0,8348  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 198300: 9914.23 Train Score: 0.00 ;0,0,0 Loss: -0.173937 Valid Score: 19110.00 ;8470,0,6490 TEST Score: \u001b[47m \u001b[37m  -1460.00 ;8640,0,6320  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 198600: 11080.30 Train Score: 0.00 ;0,0,0 Loss: -0.174012 Valid Score: 1750.00 ;6433,0,8527 TEST Score: \u001b[47m \u001b[37m  6890.00 ;6469,0,8491  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 198900: 10355.37 Train Score: 0.00 ;0,0,0 Loss: -0.173920 Valid Score: 3260.00 ;8082,0,6878 TEST Score: \u001b[47m \u001b[37m  30510.00 ;8216,0,6744  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 199200: 12263.70 Train Score: 0.00 ;0,0,0 Loss: -0.173804 Valid Score: 3420.00 ;7834,0,7126 TEST Score: \u001b[47m \u001b[37m  22740.00 ;7955,0,7005  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 199500: 12646.50 Train Score: 2597585.00 ;17293,0,22667 Loss: -0.174239 Valid Score: 20550.00 ;6341,0,8619 TEST Score: \u001b[47m \u001b[37m  -12760.00 ;6401,0,8559  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 199800: 11097.03 Train Score: 0.00 ;0,0,0 Loss: -0.173845 Valid Score: -13750.00 ;8688,0,6272 TEST Score: \u001b[47m \u001b[37m  16700.00 ;8791,0,6169  \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    env.data_init(df_train,df_valid,df_test,df_test.iloc[:,:4])\n",
    "    obs=env.reset()\n",
    "    obs_v=obs.values\n",
    "\n",
    "    reward_sum = 0\n",
    "    num_actions = 3\n",
    "\n",
    "    states = np.empty(0).reshape(0,40,4)\n",
    "    actions = np.empty(0).reshape(0,1)\n",
    "    rewards = np.empty(0).reshape(0,1)\n",
    "    discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "    num_episode = 0\n",
    "    losses = []\n",
    "\n",
    "    while num_episode < num_episodes:\n",
    "        state = np.reshape(obs_v, [1, 40,4])\n",
    "        predict = model_predict.predict([state])[0]\n",
    "        action = np.random.choice(range(num_actions),p=predict)\n",
    "\n",
    "        states = np.vstack([states, state])\n",
    "        actions = np.vstack([actions, action])\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs_v=obs.values\n",
    "        reward_sum += reward\n",
    "        rewards = np.vstack([rewards, reward])\n",
    "\n",
    "        if done:\n",
    "            discounted_rewards_episode = discount_rewards(rewards, gamma)       \n",
    "            discounted_rewards = np.vstack([discounted_rewards, discounted_rewards_episode])\n",
    "\n",
    "            rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "            if (num_episode + 1) % batch_size == 0:\n",
    "                rmax=max( abs(discounted_rewards.min()),abs(discounted_rewards.max()))\n",
    "                discounted_rewards /= rmax\n",
    "                discounted_rewards = discounted_rewards.squeeze()\n",
    "                actions = actions.squeeze().astype(int)\n",
    "\n",
    "                actions_train = np.zeros([len(actions), num_actions])\n",
    "                actions_train[np.arange(len(actions)), actions] = 1\n",
    "\n",
    "                loss = model_train.train_on_batch([states, discounted_rewards], actions_train)\n",
    "                losses.append(loss)\n",
    "\n",
    "                states = np.empty(0).reshape(0,40,4)\n",
    "                actions = np.empty(0).reshape(0,1)\n",
    "                discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "\n",
    "            if (num_episode + 1) % print_every == 0:\n",
    "                # Print status\n",
    "                score_train,tact_train = (0,np.array([0,0,0]))\n",
    "                if (num_episode + 1) % print_every_train == 0:\n",
    "                    score_train,tact_train = score_train_model(model_predict)\n",
    "                score_valid,tact_valid = score_valid_model(model_predict)\n",
    "                score_test,tact_test = score_test_model(model_predict)\n",
    "\n",
    "                print(\"Average reward for training episode {}: {:0.2f} Train Score: {:0.2f} ;{},{},{} Loss: {:0.6f} Valid Score: {:0.2f} ;{},{},{} TEST Score: \\33[47m \\33[37m  {:0.2f} ;{},{},{}  \\033[0m \".format(\n",
    "                (num_episode + 1), reward_sum/print_every, \n",
    "                score_train,tact_train[0],tact_train[1],tact_train[2],\n",
    "\n",
    "                np.mean(losses[-print_every:]),\n",
    "                score_valid,tact_valid[0],tact_valid[1],tact_valid[2],\n",
    "                score_test,tact_test[0],tact_test[1],tact_test[2],\n",
    "\n",
    "                ))\n",
    "\n",
    "                model_predict.save('models/model_v1big_{}'.format(num_episode))\n",
    "                reward_sum = 0\n",
    "\n",
    "            num_episode += 1\n",
    "            obs=env.reset()\n",
    "            obs_v=obs.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot (np.mean(losses[-print_every:]))\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
