{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:20.862421Z",
     "start_time": "2019-12-16T15:33:19.175073Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('data\\SPFB.RTS_090601_190813.txt',sep=',')\n",
    "df['Timestamp'] = df[\"<DATE>\"].astype(str) + df[\"<TIME>\"].astype(str)\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'],format='%Y%m%d%H%M%S')\n",
    "df.set_index('Timestamp',inplace=True)\n",
    "df.drop(['<DATE>','<TIME>'],axis=1,inplace=True)\n",
    "df.columns = ['Open','High','Low','Close','Vol']\n",
    "df.drop('Vol',axis=1,inplace=True)\n",
    "import os\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:20.986693Z",
     "start_time": "2019-12-16T15:33:20.929817Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: how in .resample() is deprecated\n",
      "the new syntax is .resample(...)..apply(<func>)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "conversion = {'Open' : 'first', 'High' : 'max', 'Low' : 'min', 'Close' : 'last'}\n",
    "df=df.resample('30Min', how=conversion)\n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:21.142277Z",
     "start_time": "2019-12-16T15:33:21.047530Z"
    }
   },
   "outputs": [],
   "source": [
    "def bar_norm(df):\n",
    "    t2=df.copy()\n",
    "    temp=df.copy()\n",
    "    temp['dif_close_1_cum']=(temp['Close']-temp['Close'].shift()).shift().cumsum()\n",
    "    t2['Open_std']=t2['Open'] - temp['dif_close_1_cum']\n",
    "    t2['High_std']=t2['High'] - temp['dif_close_1_cum']\n",
    "    t2['Low_std']=t2['Low'] - temp['dif_close_1_cum']\n",
    "    t2['Close_std']=t2['Close'] - temp['dif_close_1_cum']\n",
    "    t2['Open_std']=t2['Open_std'] - temp['Close'][0]\n",
    "    t2['High_std']=t2['High_std'] - temp['Close'][0]\n",
    "    t2['Low_std']=t2['Low_std'] - temp['Close'][0]\n",
    "    t2['Close_std']=t2['Close_std'] - temp['Close'][0]\n",
    "    #t2=t2-temp['Close'][0]\n",
    "    t2=t2.iloc[2:]\n",
    "    return t2\n",
    "\n",
    "def bar_norm_all(df):\n",
    "    df=bar_norm(df)\n",
    "    df['std']=df['Close_std'].rolling(1250).std()\n",
    "    df.dropna(inplace=True)\n",
    "    df['Open_std']=df['Open_std']/df['std']\n",
    "    df['High_std']=df['High_std']/df['std']\n",
    "    df['Low_std']=df['Low_std']/df['std']\n",
    "    df['Close_std']=df['Close_std']/df['std']\n",
    "    df.drop('std',axis=1,inplace=True)\n",
    "    return df\n",
    "df=bar_norm_all(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:21.208073Z",
     "start_time": "2019-12-16T15:33:21.204084Z"
    }
   },
   "outputs": [],
   "source": [
    "df=df[-70000:]\n",
    "df_train=df[:-30000]\n",
    "df_valid=df[-30000:-15000]\n",
    "df_test=df[-15000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:21.290852Z",
     "start_time": "2019-12-16T15:33:21.272900Z"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import gym_RTStrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:21.456737Z",
     "start_time": "2019-12-16T15:33:21.450751Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1.2\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Trade-v1')\n",
    "env.data_init(df_train,df_valid,df_test,df_test.iloc[:,:4])\n",
    "obs=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:23.654160Z",
     "start_time": "2019-12-16T15:33:22.019615Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.layers as layers\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "from keras.initializers import glorot_uniform\n",
    "\n",
    "\n",
    "def get_policy_model(lr,dimen=(40,4)):\n",
    "    inp = layers.Input(shape=(40,4),name=\"input_x\")\n",
    "    adv = layers.Input(shape=[1], name=\"advantages\")\n",
    "    \n",
    "    x = layers.Conv1D (16,3,activation='relu')(inp)\n",
    "    x = layers.Conv1D (24,3,activation='relu')(x)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Conv1D (32,3,activation='relu')(x)\n",
    "    x = layers.Conv1D (48,3,activation='relu')(x)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Conv1D (64,3, activation='relu')(x)\n",
    "    x = layers.Conv1D (96,3, activation='relu')(x)\n",
    "    x = layers.MaxPool1D(2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(30,activation='relu')(x)\n",
    "    out = layers.Dense(3,activation='softmax')(x)\n",
    "    \n",
    "\n",
    "    def custom_loss(y_true, y_pred):\n",
    "        log_lik = K.log(y_true * (y_true - y_pred) + (1 - y_true) * (y_true + y_pred) + K.epsilon())\n",
    "        return K.mean(log_lik * adv, keepdims=True)\n",
    "        \n",
    "    model_train = Model(inputs=[inp, adv], outputs=out)\n",
    "    model_train.compile(loss=custom_loss, optimizer=Adam(lr))\n",
    "    model_predict = Model(inputs=[inp], outputs=out)\n",
    "    return model_train, model_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "#print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:23.735264Z",
     "start_time": "2019-12-16T15:33:23.732248Z"
    }
   },
   "outputs": [],
   "source": [
    "def discount_rewards(r, gamma=0.8):\n",
    "    prior = 0\n",
    "    out = []\n",
    "    for val in r[::-1]:\n",
    "        new_val = val + prior * gamma\n",
    "        out.append(new_val)\n",
    "        prior = new_val\n",
    "    return np.array(out[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:23.927359Z",
     "start_time": "2019-12-16T15:33:23.915391Z"
    }
   },
   "outputs": [],
   "source": [
    "def score_train_model(model):   \n",
    "    obs = env.reset_to_trainScore()\n",
    "    obs_v=obs.values\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()        \n",
    "        state = np.reshape(obs_v, [1, 40, 4])\n",
    "        predict = model.predict([state])[0]\n",
    "        action = np.argmax(predict)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs_v=obs.values\n",
    "        if done:\n",
    "            print('done')\n",
    "            break\n",
    "    \n",
    "    return (env.net,env.action_count)\n",
    "\n",
    "def score_valid_model(model):\n",
    "    obs = env.reset_to_validScore()\n",
    "    obs_v=obs.values\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()        \n",
    "        state = np.reshape(obs_v, [1, 40, 4])\n",
    "        predict = model.predict([state])[0]\n",
    "        action = np.argmax(predict)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs_v=obs.values\n",
    "        if done:\n",
    "            print('done')\n",
    "            break\n",
    "    return (env.net,env.action_count)\n",
    "\n",
    "def score_test_model(model):\n",
    "    obs = env.reset_to_testScore()\n",
    "    obs_v=obs.values\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()        \n",
    "        state = np.reshape(obs_v, [1, 40, 4])\n",
    "        predict = model.predict([state])[0]\n",
    "        action = np.argmax(predict)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs_v=obs.values\n",
    "        if done:\n",
    "            print('done')\n",
    "            break\n",
    "    return (env.net,env.action_count)\n",
    "\n",
    "def score_testReal_model(model):\n",
    "    obs = env.reset_to_testReal()\n",
    "    obs = bar_norm_all(obs)\n",
    "    obs=obs.iloc[-40:,4:]\n",
    "    obs_v=obs.values\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()        \n",
    "        state = np.reshape(obs_v, [1, 40, 4])\n",
    "        predict = model.predict([state])[0]\n",
    "        action = np.argmax(predict)\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs = bar_norm_all(obs)\n",
    "        obs=obs.iloc[-40:,4:]\n",
    "        obs_v=obs.values\n",
    "        if done:\n",
    "            print('done')\n",
    "            break\n",
    "    return (env.net,env.action_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:26.190579Z",
     "start_time": "2019-12-16T15:33:26.187587Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gamma = 0\n",
    "dimen = (40,4)\n",
    "print_every = 300\n",
    "batch_size = 100\n",
    "num_episodes = 15000\n",
    "render = False\n",
    "lr = 1e-3\n",
    "print_every_train = print_every*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T15:33:26.839682Z",
     "start_time": "2019-12-16T15:33:26.707621Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0113 17:08:38.530238 13424 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0113 17:08:38.546268 13424 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0113 17:08:38.547272 13424 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0113 17:08:38.580261 13424 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0113 17:08:38.688261 13424 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0113 17:08:38.695265 13424 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:1521: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_x (InputLayer)         (None, 40, 4)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 38, 16)            208       \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 36, 24)            1176      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 18, 24)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 16, 32)            2336      \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 14, 48)            4656      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 7, 48)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 5, 64)             9280      \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 3, 96)             18528     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 1, 96)             0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 96)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                2910      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 93        \n",
      "=================================================================\n",
      "Total params: 39,187\n",
      "Trainable params: 39,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_train, model_predict = get_policy_model(lr,dimen)\n",
    "model_predict.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-16T19:08:57.973244Z",
     "start_time": "2019-12-16T15:33:29.196773Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0113 17:18:32.924329 13424 deprecation_wrapper.py:119] From C:\\ProgramData\\Anaconda3\\envs\\tensorflow_gpuenv\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:2741: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n",
      "done\n",
      "Average reward for training episode 300: -545.14 Train Score: 0.00 ;0,0,0 Loss: 0.000467 Valid Score: -15029.00 ;0,14949,11 TEST Score: \u001b[47m \u001b[37m  -14792.00 ;0,14952,8  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 600: -977.77 Train Score: 0.00 ;0,0,0 Loss: 0.000698 Valid Score: -14960.00 ;0,14960,0 TEST Score: \u001b[47m \u001b[37m  -14960.00 ;0,14960,0  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 900: -697.89 Train Score: 0.00 ;0,0,0 Loss: 0.000707 Valid Score: -14960.00 ;0,14960,0 TEST Score: \u001b[47m \u001b[37m  -14960.00 ;0,14960,0  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 1200: -517.81 Train Score: 0.00 ;0,0,0 Loss: 0.000648 Valid Score: -14960.00 ;0,14960,0 TEST Score: \u001b[47m \u001b[37m  -14960.00 ;0,14960,0  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 1500: -687.34 Train Score: -39960.00 ;0,39960,0 Loss: 0.000637 Valid Score: -14960.00 ;0,14960,0 TEST Score: \u001b[47m \u001b[37m  -14960.00 ;0,14960,0  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 1800: -482.41 Train Score: 0.00 ;0,0,0 Loss: 0.000590 Valid Score: 802.00 ;0,14288,672 TEST Score: \u001b[47m \u001b[37m  17207.00 ;0,14303,657  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 2100: -257.63 Train Score: 0.00 ;0,0,0 Loss: 0.000483 Valid Score: -33162.00 ;0,6082,8878 TEST Score: \u001b[47m \u001b[37m  17535.00 ;0,6105,8855  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 2400: 302.26 Train Score: 0.00 ;0,0,0 Loss: 0.000315 Valid Score: -7935.00 ;10,115,14835 TEST Score: \u001b[47m \u001b[37m  27541.00 ;13,139,14808  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 2700: -650.25 Train Score: 0.00 ;0,0,0 Loss: 0.000356 Valid Score: -5036.00 ;207,216,14537 TEST Score: \u001b[47m \u001b[37m  32525.00 ;213,225,14522  \u001b[0m \n",
      "done\n",
      "done\n",
      "done\n",
      "Average reward for training episode 3000: -535.10 Train Score: 188905.00 ;5541,820,33599 Loss: 0.000228 Valid Score: -50948.00 ;1956,288,12716 TEST Score: \u001b[47m \u001b[37m  23405.00 ;1970,345,12645  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 3300: 545.53 Train Score: 0.00 ;0,0,0 Loss: -0.000068 Valid Score: -48470.00 ;1701,10,13249 TEST Score: \u001b[47m \u001b[37m  30315.00 ;1720,15,13225  \u001b[0m \n",
      "done\n",
      "done\n",
      "Average reward for training episode 3600: 289.28 Train Score: 0.00 ;0,0,0 Loss: -0.000419 Valid Score: -59613.00 ;2110,3,12847 TEST Score: \u001b[47m \u001b[37m  39778.00 ;2150,2,12808  \u001b[0m \n"
     ]
    }
   ],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    env.data_init(df_train,df_valid,df_test,df_test.iloc[:,:4])\n",
    "    obs=env.reset()\n",
    "    obs_v=obs.values\n",
    "\n",
    "    reward_sum = 0\n",
    "    num_actions = 3\n",
    "\n",
    "    states = np.empty(0).reshape(0,40,4)\n",
    "    actions = np.empty(0).reshape(0,1)\n",
    "    rewards = np.empty(0).reshape(0,1)\n",
    "    discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "    num_episode = 0\n",
    "    losses = []\n",
    "\n",
    "    while num_episode < num_episodes:\n",
    "        state = np.reshape(obs_v, [1, 40,4])\n",
    "        predict = model_predict.predict([state])[0]\n",
    "        action = np.random.choice(range(num_actions),p=predict)\n",
    "\n",
    "        states = np.vstack([states, state])\n",
    "        actions = np.vstack([actions, action])\n",
    "\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        obs_v=obs.values\n",
    "        reward_sum += reward\n",
    "        rewards = np.vstack([rewards, reward])\n",
    "\n",
    "        if done:\n",
    "            discounted_rewards_episode = discount_rewards(rewards, gamma)       \n",
    "            discounted_rewards = np.vstack([discounted_rewards, discounted_rewards_episode])\n",
    "\n",
    "            rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "            if (num_episode + 1) % batch_size == 0:\n",
    "                rmax=max( abs(discounted_rewards.min()),abs(discounted_rewards.max()))\n",
    "                discounted_rewards /= rmax\n",
    "                discounted_rewards = discounted_rewards.squeeze()\n",
    "                actions = actions.squeeze().astype(int)\n",
    "\n",
    "                actions_train = np.zeros([len(actions), num_actions])\n",
    "                actions_train[np.arange(len(actions)), actions] = 1\n",
    "\n",
    "                loss = model_train.train_on_batch([states, discounted_rewards], actions_train)\n",
    "                losses.append(loss)\n",
    "\n",
    "                states = np.empty(0).reshape(0,40,4)\n",
    "                actions = np.empty(0).reshape(0,1)\n",
    "                discounted_rewards = np.empty(0).reshape(0,1)\n",
    "\n",
    "\n",
    "            if (num_episode + 1) % print_every == 0:\n",
    "                # Print status\n",
    "                score_train,tact_train = (0,np.array([0,0,0]))\n",
    "                if (num_episode + 1) % print_every_train == 0:\n",
    "                    score_train,tact_train = score_train_model(model_predict)\n",
    "                score_valid,tact_valid = score_valid_model(model_predict)\n",
    "                score_test,tact_test = score_test_model(model_predict)\n",
    "\n",
    "                print(\"Average reward for training episode {}: {:0.2f} Train Score: {:0.2f} ;{},{},{} Loss: {:0.6f} Valid Score: {:0.2f} ;{},{},{} TEST Score: \\33[47m \\33[37m  {:0.2f} ;{},{},{}  \\033[0m \".format(\n",
    "                (num_episode + 1), reward_sum/print_every, \n",
    "                score_train,tact_train[0],tact_train[1],tact_train[2],\n",
    "\n",
    "                np.mean(losses[-print_every:]),\n",
    "                score_valid,tact_valid[0],tact_valid[1],tact_valid[2],\n",
    "                score_test,tact_test[0],tact_test[1],tact_test[2],\n",
    "\n",
    "                ))\n",
    "\n",
    "                model_predict.save('models/model_v1big_{}'.format(num_episode))\n",
    "                reward_sum = 0\n",
    "\n",
    "            num_episode += 1\n",
    "            obs=env.reset()\n",
    "            obs_v=obs.values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot (np.mean(losses[-print_every:]))\n",
    "plt.show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
